{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an SVM or XGBoost model\n",
    "\n",
    "The focus of this is to train a model in one dataset and try to predict in another from a fifferent cell line. My interest here is to check if I can improve our ability to classify by using the k-mer scores as follows:\n",
    " - Create a possitive and a negative set from one ChIP-seq cell line\n",
    "     - Score the sequence using:\n",
    "         - E-scores\n",
    "         - frequency difference\n",
    "         - a kmer based model generated by training the sequences / using kmer counts of all the 8-mers in each set\n",
    "     - Use the model generated to classify a given set of sequences\n",
    " \n",
    " The next question is, wha kind of codes do we need:\n",
    "     1. A scoring function for all the sequences\n",
    "     2. A quick way to get the counts of the k-mers / have a look at how some previous challenges have used this approach\n",
    "     3. A quick way to get the DNA-shape features: This will require choosing a portion of the highest scoring window to use\n",
    "     4. Test the above features first, and then see if we can expand this to other features\n",
    "     \n",
    "     \n",
    "To be able to achieve all the above, we will need to get an indepth understanding of what we are doing, the algorithm we will use, and how much we will end up doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import  exp\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pybedtools\n",
    "import pyBigWig\n",
    "import pysam\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Main SVM module and grid search function\n",
    "from sklearn import svm, grid_search\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "#For partitioning the data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "\n",
    "#Libsvm format data loading\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "#Accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, auc\n",
    "\n",
    "# Creating an learning pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pbm_dnase_xgboost import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First set the path to essential, but large files\n",
    "\n",
    "### 1. The DNA Shape files\n",
    "Downloaded from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_path = \"/home/kipkurui/Dream_challenge/DNAShape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The human genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_genome = \"/home/kipkurui/Dream_challenge/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Uniformly processed ChIP-seq peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chipseq_path = \"/home/kipkurui/Project/MARS/Data/ChIP-seq/Downloaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create kmer dictionaries for the features of interest\n",
    "We have two options here:\n",
    "1. Backround noise scalled in a similar maner to sticky k-mers \n",
    "1. Preferred k-mers max normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dn_hg_dict, kmer_name = get_kmer_dict_rev(\"dn_hg_max_normalized.txt\", \"test\")\n",
    "\n",
    "dn_hg_dict2, kmer_name = get_kmer_dict_rev(\"hg_dn_backround_noise_minmax.txt\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test = pd.read_table(\"rewighed_hg_dn_scores\")\n",
    "# minmax_df = pd.read_table(minmax, header=None)\n",
    "# test[\"E-score\"] = minmax_df[1]\n",
    "# test.to_csv(\"dn_hg_max_normalized.txt\", sep=\"\\t\", header=True, index=False)\n",
    "# dn_hg_dict, kmer_name = get_kmer_dict_rev(\"rewighed_hg_dn_scores\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Where best can I put these files? These also need prope names\n",
    "\n",
    "\n",
    "\n",
    "# kmerscore = \"hg_dn_backround_noise_minmax.txt\"\n",
    "# test = pd.read_table(kmerscore, header=None, index_col=0)\n",
    "# dn_hg_dict2 = test[1].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Score the sequences of interest\n",
    "\n",
    "#### a) K_mer score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all data prepapration steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feature_df(tf, pos):\n",
    "    \"\"\"\n",
    "    Given a TF and the position of the peak file of interest\n",
    "    Creat a DataFrame with all the coordinates\n",
    "    \n",
    "    This is the main Feature Vector\n",
    "    \"\"\"\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    ## Calculate all the necessary features\n",
    "    #E_score_combined = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "    feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "    feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete Feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_list = ['max_kmer_score','dnase','kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train a model using the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ap2\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File Data/repeat_sites.bed does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9a296fe15d4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpybedtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfeature_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mfeature_frame_p\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrim_to_p\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_feature_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrim_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrim_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-998e25ad047f>\u001b[0m in \u001b[0;36mget_feature_df\u001b[1;34m(tf, pos)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpeak_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_peak_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcombined_bed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_combined_bed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeak_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mE_score_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkmer_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_contigmers_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_contigmers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/Project/PBM_DNase/code/pbm_dnase_xgboost.py\u001b[0m in \u001b[0;36mget_combined_bed\u001b[1;34m(peak)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[0msequences\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \"\"\"\n\u001b[1;32m--> 471\u001b[1;33m     \u001b[0mpeak_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_bed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bed_from_peaks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeak\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[0mtrim_to\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeak_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_bed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/Project/PBM_DNase/code/pbm_dnase_xgboost.py\u001b[0m in \u001b[0;36mget_bed_from_peaks\u001b[1;34m(peak, width, downstream_distance)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;31m# Eliminate repeat masked regions from the bed file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m     \u001b[0mpeak_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_repeats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeak_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.to_csv(pos_bed_out, index=None, header=None, sep=\"\\t\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m     \u001b[0mneg_bed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_repeats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_bed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.to_csv(neg_bed_out, index=None, header=None, sep=\"\\t\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/Project/PBM_DNase/code/pbm_dnase_xgboost.py\u001b[0m in \u001b[0;36mremove_repeats\u001b[1;34m(dfs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[0mcoordinates\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mfall\u001b[0m \u001b[0mwithin\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrepeat\u001b[0m \u001b[0mmasked\u001b[0m \u001b[0msections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m--> 456\u001b[1;33m     \u001b[0mrepeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/repeat_sites.bed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m     \u001b[0mrepeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpybedtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBedTool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File Data/repeat_sites.bed does not exist"
     ]
    }
   ],
   "source": [
    "with open(\"../Results/Machine_Learning/TF_scores_feature_importance_recursive_pop.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "    tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    for j in feat_list:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in [\"Ap2\"]:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "        pybedtools.cleanup()\n",
    "        \n",
    "        feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in feat_list:\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kmer_score</th>\n",
       "      <th>max_kmer_score_pos</th>\n",
       "      <th>max_kmer_score</th>\n",
       "      <th>dnase</th>\n",
       "      <th>phatsCons</th>\n",
       "      <th>phyloP100way</th>\n",
       "      <th>dn_hg_score</th>\n",
       "      <th>dn_hg_score2</th>\n",
       "      <th>tss_dist</th>\n",
       "      <th>ProT_0</th>\n",
       "      <th>...</th>\n",
       "      <th>HelT_6</th>\n",
       "      <th>HelT_7</th>\n",
       "      <th>Roll_0</th>\n",
       "      <th>Roll_1</th>\n",
       "      <th>Roll_2</th>\n",
       "      <th>Roll_3</th>\n",
       "      <th>Roll_4</th>\n",
       "      <th>Roll_5</th>\n",
       "      <th>Roll_6</th>\n",
       "      <th>Roll_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.25113</td>\n",
       "      <td>2.42482</td>\n",
       "      <td>0.470156</td>\n",
       "      <td>607</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.33325</td>\n",
       "      <td>(0.570028395556, 76)</td>\n",
       "      <td>(-0.37105261446, 55)</td>\n",
       "      <td>17977</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-2.13</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-1.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.96527</td>\n",
       "      <td>2.04187</td>\n",
       "      <td>0.399615</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>-1.35387</td>\n",
       "      <td>(0.919298907842, 33)</td>\n",
       "      <td>(-0.603785220001, 62)</td>\n",
       "      <td>2344</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>5.91</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.93557</td>\n",
       "      <td>2.38779</td>\n",
       "      <td>0.403294</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.98925</td>\n",
       "      <td>1.2575</td>\n",
       "      <td>(0.680010615819, 1)</td>\n",
       "      <td>(-0.544525970979, 75)</td>\n",
       "      <td>5508</td>\n",
       "      <td>-5.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>4.64</td>\n",
       "      <td>-5.08</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>5.91</td>\n",
       "      <td>-3.79</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.82103</td>\n",
       "      <td>2.54136</td>\n",
       "      <td>0.476339</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.99563</td>\n",
       "      <td>(0.597967077756, -2)</td>\n",
       "      <td>(-0.296024991639, 39)</td>\n",
       "      <td>18511</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>-2.21</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-3.61</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.66444</td>\n",
       "      <td>2.74604</td>\n",
       "      <td>0.45247</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>-0.367375</td>\n",
       "      <td>(0.597967077756, 10)</td>\n",
       "      <td>(-0.304491754359, 65)</td>\n",
       "      <td>1402</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>-0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.9784</td>\n",
       "      <td>1.57229</td>\n",
       "      <td>0.474166</td>\n",
       "      <td>731</td>\n",
       "      <td>0.006625</td>\n",
       "      <td>0.284125</td>\n",
       "      <td>(0.559842173341, 43)</td>\n",
       "      <td>(-0.335155645376, 72)</td>\n",
       "      <td>15585</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.38</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>3.17</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>-3.38</td>\n",
       "      <td>-3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.74006</td>\n",
       "      <td>2.62295</td>\n",
       "      <td>0.403799</td>\n",
       "      <td>264</td>\n",
       "      <td>0.015875</td>\n",
       "      <td>0.544125</td>\n",
       "      <td>(0.336447603865, -4)</td>\n",
       "      <td>(0.702520717311, 10)</td>\n",
       "      <td>114505</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>3.49</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.32935</td>\n",
       "      <td>1.18539</td>\n",
       "      <td>0.398044</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-0.5655</td>\n",
       "      <td>(0.663213779445, 88)</td>\n",
       "      <td>(-0.526873725128, 80)</td>\n",
       "      <td>231381</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>...</td>\n",
       "      <td>5.64</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>3.95</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>-4.42</td>\n",
       "      <td>-3.61</td>\n",
       "      <td>5.64</td>\n",
       "      <td>-0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.95517</td>\n",
       "      <td>1.74622</td>\n",
       "      <td>0.392525</td>\n",
       "      <td>643</td>\n",
       "      <td>0.239125</td>\n",
       "      <td>0.087125</td>\n",
       "      <td>(0.501852285165, 75)</td>\n",
       "      <td>(-0.411183904292, 51)</td>\n",
       "      <td>9635</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-4.52</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>3.06</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>3.98</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.88431</td>\n",
       "      <td>2.57128</td>\n",
       "      <td>0.480663</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.89725</td>\n",
       "      <td>(0.288777485314, 66)</td>\n",
       "      <td>(-0.233166867884, 23)</td>\n",
       "      <td>3431</td>\n",
       "      <td>3.58</td>\n",
       "      <td>...</td>\n",
       "      <td>6.73</td>\n",
       "      <td>-3.89</td>\n",
       "      <td>3.58</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>4.78</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-3.63</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>6.73</td>\n",
       "      <td>-3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.05054</td>\n",
       "      <td>2.47441</td>\n",
       "      <td>0.446877</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1.62425</td>\n",
       "      <td>(0.61619056596, 88)</td>\n",
       "      <td>(-0.313498474288, 55)</td>\n",
       "      <td>6000</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>-2.42</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>-2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2.12328</td>\n",
       "      <td>-0.0800726</td>\n",
       "      <td>0.294184</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>3.21287</td>\n",
       "      <td>(0.834697313669, 10)</td>\n",
       "      <td>(-0.00845624979757, 24)</td>\n",
       "      <td>8508</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.92339</td>\n",
       "      <td>2.89211</td>\n",
       "      <td>0.472562</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.012625</td>\n",
       "      <td>0.10275</td>\n",
       "      <td>(0.242987761279, 80)</td>\n",
       "      <td>(0.00755911534536, 20)</td>\n",
       "      <td>87975</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>6.98</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.75163</td>\n",
       "      <td>2.47785</td>\n",
       "      <td>0.44402</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.996125</td>\n",
       "      <td>1.15025</td>\n",
       "      <td>(0.416216369403, 32)</td>\n",
       "      <td>(0.0969528036993, 9)</td>\n",
       "      <td>26041</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.11</td>\n",
       "      <td>6.09</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>3.58</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>4.36</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-4.11</td>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.50671</td>\n",
       "      <td>2.84419</td>\n",
       "      <td>0.446313</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>2.13825</td>\n",
       "      <td>(0.483258361201, 85)</td>\n",
       "      <td>(-0.030210543615, 16)</td>\n",
       "      <td>11969</td>\n",
       "      <td>-3.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>-3.01</td>\n",
       "      <td>7.72</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>-5.17</td>\n",
       "      <td>7.31</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-5.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.05288</td>\n",
       "      <td>2.02132</td>\n",
       "      <td>0.423344</td>\n",
       "      <td>460</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.04225</td>\n",
       "      <td>(0.207729364376, 55)</td>\n",
       "      <td>(-0.143249605594, 63)</td>\n",
       "      <td>66008</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>3.64</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-5.84034</td>\n",
       "      <td>-0.309826</td>\n",
       "      <td>0.296116</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.146625</td>\n",
       "      <td>0.376</td>\n",
       "      <td>(0.343667569598, -4)</td>\n",
       "      <td>(0.0969528036993, 7)</td>\n",
       "      <td>158629</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>-3.96</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>4.6</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-3.6296</td>\n",
       "      <td>2.04157</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>265</td>\n",
       "      <td>0.00275</td>\n",
       "      <td>0.2745</td>\n",
       "      <td>(0.928037668609, 18)</td>\n",
       "      <td>(-0.424113082019, 69)</td>\n",
       "      <td>154458</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2.77</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>6.98</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-9.60505</td>\n",
       "      <td>-0.817533</td>\n",
       "      <td>0.19587</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>0.837875</td>\n",
       "      <td>(0.663213779445, 47)</td>\n",
       "      <td>(-0.348720913679, 84)</td>\n",
       "      <td>71182</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>...</td>\n",
       "      <td>3.93</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>2.74</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>3.93</td>\n",
       "      <td>-2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-5.00812</td>\n",
       "      <td>1.77855</td>\n",
       "      <td>0.410835</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.106125</td>\n",
       "      <td>-0.155125</td>\n",
       "      <td>(0.610853867649, 27)</td>\n",
       "      <td>(-0.350694515445, 62)</td>\n",
       "      <td>25186</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>2.84</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>5.86</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-6.80954</td>\n",
       "      <td>2.68335</td>\n",
       "      <td>0.467757</td>\n",
       "      <td>734</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>-0.05725</td>\n",
       "      <td>(0.582847354665, 76)</td>\n",
       "      <td>(0.0969528036993, 84)</td>\n",
       "      <td>32195</td>\n",
       "      <td>-3.79</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-3.89</td>\n",
       "      <td>-3.79</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>2.79</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.27924</td>\n",
       "      <td>2.36999</td>\n",
       "      <td>0.417076</td>\n",
       "      <td>653</td>\n",
       "      <td>0.2865</td>\n",
       "      <td>0.77975</td>\n",
       "      <td>(0.395609756863, 21)</td>\n",
       "      <td>(-0.294433611601, 6)</td>\n",
       "      <td>30205</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>5.62</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-2.91</td>\n",
       "      <td>-2.65</td>\n",
       "      <td>4.48</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>5.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.55272</td>\n",
       "      <td>2.94312</td>\n",
       "      <td>0.469631</td>\n",
       "      <td>360</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.12075</td>\n",
       "      <td>(0.654557372589, 14)</td>\n",
       "      <td>(0.267240392175, 53)</td>\n",
       "      <td>53231</td>\n",
       "      <td>6.83</td>\n",
       "      <td>...</td>\n",
       "      <td>4.39</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>6.83</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>-3.52</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>4.39</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-5.27278</td>\n",
       "      <td>1.66496</td>\n",
       "      <td>0.421517</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>(0.649822125731, 3)</td>\n",
       "      <td>(-0.224364579171, 31)</td>\n",
       "      <td>7107</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>4.64</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>5.97</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-2.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.46528</td>\n",
       "      <td>2.64401</td>\n",
       "      <td>0.465254</td>\n",
       "      <td>487</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.114875</td>\n",
       "      <td>(0.6063393947, 12)</td>\n",
       "      <td>(-0.231791532171, 85)</td>\n",
       "      <td>315</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.72444</td>\n",
       "      <td>3.2572</td>\n",
       "      <td>0.478361</td>\n",
       "      <td>327</td>\n",
       "      <td>0.088875</td>\n",
       "      <td>-0.994375</td>\n",
       "      <td>(0.262105188039, 62)</td>\n",
       "      <td>(-0.049702837643, 50)</td>\n",
       "      <td>41490</td>\n",
       "      <td>3.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.71</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>6.73</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.767666</td>\n",
       "      <td>3.03976</td>\n",
       "      <td>0.465478</td>\n",
       "      <td>395</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>0.77375</td>\n",
       "      <td>(0.557799525129, 46)</td>\n",
       "      <td>(0.0345942843156, 57)</td>\n",
       "      <td>80783</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>...</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-4.11</td>\n",
       "      <td>6.33</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-4.06516</td>\n",
       "      <td>2.55774</td>\n",
       "      <td>0.44762</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.82913</td>\n",
       "      <td>(0.56139269972, 27)</td>\n",
       "      <td>(-0.483565360218, 73)</td>\n",
       "      <td>32584</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-4.63</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>2.77</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>4.64</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-6.9616</td>\n",
       "      <td>2.39028</td>\n",
       "      <td>0.413065</td>\n",
       "      <td>826</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.827375</td>\n",
       "      <td>(0.782802170349, 20)</td>\n",
       "      <td>(-0.644943066238, 78)</td>\n",
       "      <td>74530</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.38</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>-2.51</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-3.77</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.635035</td>\n",
       "      <td>3.409</td>\n",
       "      <td>0.478361</td>\n",
       "      <td>962</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>-0.427125</td>\n",
       "      <td>(0.576356081462, -4)</td>\n",
       "      <td>(-0.446815270639, 46)</td>\n",
       "      <td>142070</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>4.39</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18338</th>\n",
       "      <td>-4.62056</td>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.282409</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02025</td>\n",
       "      <td>-0.098875</td>\n",
       "      <td>(0.655287838323, 1)</td>\n",
       "      <td>(0.843891437452, 82)</td>\n",
       "      <td>39143</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>3.74</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18339</th>\n",
       "      <td>-1.52492</td>\n",
       "      <td>3.00725</td>\n",
       "      <td>0.439514</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>-0.534875</td>\n",
       "      <td>(0.56139269972, 44)</td>\n",
       "      <td>(-0.509956425589, -2)</td>\n",
       "      <td>23624</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>3.96</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>-2.16</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18340</th>\n",
       "      <td>-0.711928</td>\n",
       "      <td>1.59203</td>\n",
       "      <td>0.350376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.376125</td>\n",
       "      <td>(0.343373701784, 46)</td>\n",
       "      <td>(-0.0848270542109, 59)</td>\n",
       "      <td>224407</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-3.81</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>3.08</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>5.86</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>-3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18341</th>\n",
       "      <td>-9.30753</td>\n",
       "      <td>0.789041</td>\n",
       "      <td>0.415032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136125</td>\n",
       "      <td>0.628375</td>\n",
       "      <td>(0.515434199589, 71)</td>\n",
       "      <td>(0.138639287995, 86)</td>\n",
       "      <td>84277</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>6.86</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>5.95</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-4.29</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>6.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18342</th>\n",
       "      <td>-4.46803</td>\n",
       "      <td>1.45038</td>\n",
       "      <td>0.404714</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>-0.456375</td>\n",
       "      <td>(0.349815744522, 42)</td>\n",
       "      <td>(-0.436492368737, 72)</td>\n",
       "      <td>37609</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18343</th>\n",
       "      <td>-4.3161</td>\n",
       "      <td>1.78408</td>\n",
       "      <td>0.40067</td>\n",
       "      <td>182</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.00437499</td>\n",
       "      <td>(0.217427620754, 24)</td>\n",
       "      <td>(0.250305088733, 31)</td>\n",
       "      <td>39759</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-3.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>4.73</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>-0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18344</th>\n",
       "      <td>-5.05746</td>\n",
       "      <td>0.768536</td>\n",
       "      <td>0.353326</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10375</td>\n",
       "      <td>0.835375</td>\n",
       "      <td>(0.620475044627, 17)</td>\n",
       "      <td>(-0.180458617322, 9)</td>\n",
       "      <td>116494</td>\n",
       "      <td>3.17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>3.17</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>-2.69</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-4.08</td>\n",
       "      <td>7.33</td>\n",
       "      <td>-2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18345</th>\n",
       "      <td>-2.02786</td>\n",
       "      <td>0.297131</td>\n",
       "      <td>0.293325</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.132</td>\n",
       "      <td>(0.275841389139, 70)</td>\n",
       "      <td>(0.471491200668, 53)</td>\n",
       "      <td>842956</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-3.03</td>\n",
       "      <td>-7.53</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>4.51</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-3.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18346</th>\n",
       "      <td>1.95148</td>\n",
       "      <td>2.45493</td>\n",
       "      <td>0.44399</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2485</td>\n",
       "      <td>0.37475</td>\n",
       "      <td>(0.410463407059, 53)</td>\n",
       "      <td>(0.471491200668, 62)</td>\n",
       "      <td>390166</td>\n",
       "      <td>-5.85</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.77</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-5.85</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>3.59</td>\n",
       "      <td>-3.38</td>\n",
       "      <td>-2.77</td>\n",
       "      <td>-2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18347</th>\n",
       "      <td>-4.23124</td>\n",
       "      <td>1.76382</td>\n",
       "      <td>0.447679</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>(0.301073327678, 0)</td>\n",
       "      <td>(-0.138100526037, 34)</td>\n",
       "      <td>408381</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18348</th>\n",
       "      <td>-2.76003</td>\n",
       "      <td>1.14879</td>\n",
       "      <td>0.307909</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980875</td>\n",
       "      <td>1.80825</td>\n",
       "      <td>(0.478552821216, -3)</td>\n",
       "      <td>(-0.238213294231, 43)</td>\n",
       "      <td>41676</td>\n",
       "      <td>-5.01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.06</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-5.01</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.88</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>6.06</td>\n",
       "      <td>-3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18349</th>\n",
       "      <td>-6.89308</td>\n",
       "      <td>0.108977</td>\n",
       "      <td>0.157587</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>-0.348375</td>\n",
       "      <td>(0.826676956076, 1)</td>\n",
       "      <td>(-0.356312596722, 32)</td>\n",
       "      <td>21982</td>\n",
       "      <td>3.85</td>\n",
       "      <td>...</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>4.09</td>\n",
       "      <td>-4.79</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18350</th>\n",
       "      <td>-2.87543</td>\n",
       "      <td>1.39819</td>\n",
       "      <td>0.352646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.343375</td>\n",
       "      <td>(0.626264781405, 76)</td>\n",
       "      <td>(-0.534405541576, 21)</td>\n",
       "      <td>30852</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>4.23</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-3.06</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18351</th>\n",
       "      <td>-1.14868</td>\n",
       "      <td>2.1008</td>\n",
       "      <td>0.423141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064875</td>\n",
       "      <td>0.13125</td>\n",
       "      <td>(0.439678120758, 25)</td>\n",
       "      <td>(0.717193198977, 13)</td>\n",
       "      <td>73686</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-4.19</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>4.96</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>-2.64</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18352</th>\n",
       "      <td>-1.58513</td>\n",
       "      <td>0.626139</td>\n",
       "      <td>0.274735</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.42925</td>\n",
       "      <td>(0.178857586908, 42)</td>\n",
       "      <td>(0.498397777799, 33)</td>\n",
       "      <td>109070</td>\n",
       "      <td>-6.42</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-6.42</td>\n",
       "      <td>-4.15</td>\n",
       "      <td>-3.71</td>\n",
       "      <td>5.91</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>7.72</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>-3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18353</th>\n",
       "      <td>-4.46232</td>\n",
       "      <td>1.75051</td>\n",
       "      <td>0.417009</td>\n",
       "      <td>584</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>(0.535909102866, 6)</td>\n",
       "      <td>(-0.509001226895, 87)</td>\n",
       "      <td>110323</td>\n",
       "      <td>4.23</td>\n",
       "      <td>...</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>4.23</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18354</th>\n",
       "      <td>-2.50102</td>\n",
       "      <td>1.77233</td>\n",
       "      <td>0.398039</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.10175</td>\n",
       "      <td>(0.346177147553, 88)</td>\n",
       "      <td>(-0.239579849889, 28)</td>\n",
       "      <td>111751</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>4.91</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>6.17</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.86</td>\n",
       "      <td>4.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18355</th>\n",
       "      <td>2.54115</td>\n",
       "      <td>1.64129</td>\n",
       "      <td>0.403294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15125</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>(0.198842440396, 67)</td>\n",
       "      <td>(1.39954768521, 3)</td>\n",
       "      <td>28641</td>\n",
       "      <td>4.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>4.54</td>\n",
       "      <td>-3.42</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>-3.42</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18356</th>\n",
       "      <td>-1.65713</td>\n",
       "      <td>0.632714</td>\n",
       "      <td>0.325875</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.432125</td>\n",
       "      <td>(0.395003400609, 7)</td>\n",
       "      <td>(0.770923045884, 39)</td>\n",
       "      <td>111239</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-2.68</td>\n",
       "      <td>4.21</td>\n",
       "      <td>-5.42</td>\n",
       "      <td>-3.42</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18357</th>\n",
       "      <td>1.90188</td>\n",
       "      <td>2.18096</td>\n",
       "      <td>0.486154</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>(0.284298403615, 64)</td>\n",
       "      <td>(0.447881251135, 55)</td>\n",
       "      <td>98634</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>6.73</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-3.07</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>5.15</td>\n",
       "      <td>-2.44</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18358</th>\n",
       "      <td>-0.391588</td>\n",
       "      <td>1.6413</td>\n",
       "      <td>0.401825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>0.57525</td>\n",
       "      <td>(0.166213536274, 13)</td>\n",
       "      <td>(1.68137, 5)</td>\n",
       "      <td>27394</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>...</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>-3.59</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18359</th>\n",
       "      <td>1.38302</td>\n",
       "      <td>2.25319</td>\n",
       "      <td>0.378631</td>\n",
       "      <td>116</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.20175</td>\n",
       "      <td>(0.227462254781, 26)</td>\n",
       "      <td>(-0.0302049860684, 3)</td>\n",
       "      <td>85352</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>3.9</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18360</th>\n",
       "      <td>-4.02023</td>\n",
       "      <td>1.9622</td>\n",
       "      <td>0.331703</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98025</td>\n",
       "      <td>1.21638</td>\n",
       "      <td>(0.663213779445, 83)</td>\n",
       "      <td>(-0.311504847456, 73)</td>\n",
       "      <td>364424</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.83</td>\n",
       "      <td>-4.22</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>-3.42</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18361</th>\n",
       "      <td>-0.305524</td>\n",
       "      <td>1.92606</td>\n",
       "      <td>0.391554</td>\n",
       "      <td>303</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>0.336875</td>\n",
       "      <td>(0.201618306332, 21)</td>\n",
       "      <td>(-0.0541563218929, 42)</td>\n",
       "      <td>36904</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>3.79</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.35</td>\n",
       "      <td>-2.43</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>-6.54</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18362</th>\n",
       "      <td>-1.28483</td>\n",
       "      <td>0.894483</td>\n",
       "      <td>0.383014</td>\n",
       "      <td>0</td>\n",
       "      <td>0.551625</td>\n",
       "      <td>0.31025</td>\n",
       "      <td>(0.237149657829, 26)</td>\n",
       "      <td>(-0.179577388367, 45)</td>\n",
       "      <td>9343</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.06</td>\n",
       "      <td>-2.83</td>\n",
       "      <td>2.59</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>-5.15</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>6.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18363</th>\n",
       "      <td>2.59292</td>\n",
       "      <td>2.11795</td>\n",
       "      <td>0.410835</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143875</td>\n",
       "      <td>0.00600001</td>\n",
       "      <td>(0.393431381678, 71)</td>\n",
       "      <td>(1.39954768521, 53)</td>\n",
       "      <td>3811</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>6.17</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-3.55</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18364</th>\n",
       "      <td>-2.65685</td>\n",
       "      <td>2.78616</td>\n",
       "      <td>0.435015</td>\n",
       "      <td>171</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.6275</td>\n",
       "      <td>(0.620475044627, 67)</td>\n",
       "      <td>(-0.582352969041, 52)</td>\n",
       "      <td>6915</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>3.87</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>-1.93</td>\n",
       "      <td>4.64</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>-1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18365</th>\n",
       "      <td>-7.69085</td>\n",
       "      <td>2.20493</td>\n",
       "      <td>0.392525</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.47737</td>\n",
       "      <td>(0.484871250567, 47)</td>\n",
       "      <td>(-0.520256026263, 32)</td>\n",
       "      <td>20182</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-4.52</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>2.56</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>-4.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18366</th>\n",
       "      <td>-7.4982</td>\n",
       "      <td>-0.112357</td>\n",
       "      <td>0.157859</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>(0.919298907842, 31)</td>\n",
       "      <td>(-0.504944270707, 65)</td>\n",
       "      <td>3084</td>\n",
       "      <td>-3.63</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>-2.99</td>\n",
       "      <td>-3.63</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>-1.98</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>4.51</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>-2.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18367</th>\n",
       "      <td>-9.53075</td>\n",
       "      <td>-0.934534</td>\n",
       "      <td>0.214957</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>-0.1845</td>\n",
       "      <td>(0.626264781405, 19)</td>\n",
       "      <td>(-0.470377863247, 56)</td>\n",
       "      <td>498</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>4.91</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18368 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      kmer_score max_kmer_score_pos max_kmer_score dnase phatsCons  \\\n",
       "0      1.25113    2.42482            0.470156       607   0.0335     \n",
       "1     -2.96527    2.04187            0.399615       1000  0.000125   \n",
       "2     -4.93557    2.38779            0.403294       1000  0.98925    \n",
       "3      2.82103    2.54136            0.476339       1000  1          \n",
       "4     -5.66444    2.74604            0.45247        1000  0.00025    \n",
       "5     -1.9784     1.57229            0.474166       731   0.006625   \n",
       "6     -1.74006    2.62295            0.403799       264   0.015875   \n",
       "7     -2.32935    1.18539            0.398044       1000  0.0005     \n",
       "8     -2.95517    1.74622            0.392525       643   0.239125   \n",
       "9     -1.88431    2.57128            0.480663       1000  0          \n",
       "10     1.05054    2.47441            0.446877       1000  0.377      \n",
       "11    -2.12328   -0.0800726          0.294184       1000  0.999875   \n",
       "12     5.92339    2.89211            0.472562       1000  0.012625   \n",
       "13     2.75163    2.47785            0.44402        1000  0.996125   \n",
       "14     4.50671    2.84419            0.446313       1000  0.999125   \n",
       "15    -1.05288    2.02132            0.423344       460   0.001125   \n",
       "16    -5.84034   -0.309826           0.296116       1000  0.146625   \n",
       "17    -3.6296     2.04157            0.389569       265   0.00275    \n",
       "18    -9.60505   -0.817533           0.19587        1000  0.522625   \n",
       "19    -5.00812    1.77855            0.410835       1000  0.106125   \n",
       "20    -6.80954    2.68335            0.467757       734   0.00225    \n",
       "21     3.27924    2.36999            0.417076       653   0.2865     \n",
       "22     1.55272    2.94312            0.469631       360   0.050625   \n",
       "23    -5.27278    1.66496            0.421517       1000  0.00025    \n",
       "24     3.46528    2.64401            0.465254       487   0.00025    \n",
       "25    -1.72444    3.2572             0.478361       327   0.088875   \n",
       "26    -0.767666   3.03976            0.465478       395   0.1685     \n",
       "27    -4.06516    2.55774            0.44762        1000  1          \n",
       "28    -6.9616     2.39028            0.413065       826   0          \n",
       "29     0.635035   3.409              0.478361       962   0.000125   \n",
       "...         ...     ...                   ...       ...        ...   \n",
       "18338 -4.62056    0.484133           0.282409       0     0.02025    \n",
       "18339 -1.52492    3.00725            0.439514       1000  0.00025    \n",
       "18340 -0.711928   1.59203            0.350376       0     0          \n",
       "18341 -9.30753    0.789041           0.415032       0     0.136125   \n",
       "18342 -4.46803    1.45038            0.404714       0     0.002625   \n",
       "18343 -4.3161     1.78408            0.40067        182   0.001125   \n",
       "18344 -5.05746    0.768536           0.353326       0     0.10375    \n",
       "18345 -2.02786    0.297131           0.293325       0     0.002625   \n",
       "18346  1.95148    2.45493            0.44399        0     0.2485     \n",
       "18347 -4.23124    1.76382            0.447679       0     0.000625   \n",
       "18348 -2.76003    1.14879            0.307909       0     0.980875   \n",
       "18349 -6.89308    0.108977           0.157587       0     0.009375   \n",
       "18350 -2.87543    1.39819            0.352646       0     0.00125    \n",
       "18351 -1.14868    2.1008             0.423141       0     0.064875   \n",
       "18352 -1.58513    0.626139           0.274735       0     0.1635     \n",
       "18353 -4.46232    1.75051            0.417009       584   0.0005     \n",
       "18354 -2.50102    1.77233            0.398039       0     0.000875   \n",
       "18355  2.54115    1.64129            0.403294       0     0.15125    \n",
       "18356 -1.65713    0.632714           0.325875       177   0          \n",
       "18357  1.90188    2.18096            0.486154       0     0.000875   \n",
       "18358 -0.391588   1.6413             0.401825       0     0.016875   \n",
       "18359  1.38302    2.25319            0.378631       116   0.001375   \n",
       "18360 -4.02023    1.9622             0.331703       0     0.98025    \n",
       "18361 -0.305524   1.92606            0.391554       303   0.006125   \n",
       "18362 -1.28483    0.894483           0.383014       0     0.551625   \n",
       "18363  2.59292    2.11795            0.410835       0     0.143875   \n",
       "18364 -2.65685    2.78616            0.435015       171   0.4545     \n",
       "18365 -7.69085    2.20493            0.392525       189   0          \n",
       "18366 -7.4982    -0.112357           0.157859       0     0.006375   \n",
       "18367 -9.53075   -0.934534           0.214957       1000  0.00025    \n",
       "\n",
       "      phyloP100way           dn_hg_score             dn_hg_score2 tss_dist  \\\n",
       "0      0.33325      (0.570028395556, 76)  (-0.37105261446, 55)     17977     \n",
       "1     -1.35387      (0.919298907842, 33)  (-0.603785220001, 62)    2344      \n",
       "2      1.2575       (0.680010615819, 1)   (-0.544525970979, 75)    5508      \n",
       "3      2.99563      (0.597967077756, -2)  (-0.296024991639, 39)    18511     \n",
       "4     -0.367375     (0.597967077756, 10)  (-0.304491754359, 65)    1402      \n",
       "5      0.284125     (0.559842173341, 43)  (-0.335155645376, 72)    15585     \n",
       "6      0.544125     (0.336447603865, -4)  (0.702520717311, 10)     114505    \n",
       "7     -0.5655       (0.663213779445, 88)  (-0.526873725128, 80)    231381    \n",
       "8      0.087125     (0.501852285165, 75)  (-0.411183904292, 51)    9635      \n",
       "9     -0.89725      (0.288777485314, 66)  (-0.233166867884, 23)    3431      \n",
       "10     1.62425      (0.61619056596, 88)   (-0.313498474288, 55)    6000      \n",
       "11     3.21287      (0.834697313669, 10)  (-0.00845624979757, 24)  8508      \n",
       "12     0.10275      (0.242987761279, 80)  (0.00755911534536, 20)   87975     \n",
       "13     1.15025      (0.416216369403, 32)  (0.0969528036993, 9)     26041     \n",
       "14     2.13825      (0.483258361201, 85)  (-0.030210543615, 16)    11969     \n",
       "15     0.04225      (0.207729364376, 55)  (-0.143249605594, 63)    66008     \n",
       "16     0.376        (0.343667569598, -4)  (0.0969528036993, 7)     158629    \n",
       "17     0.2745       (0.928037668609, 18)  (-0.424113082019, 69)    154458    \n",
       "18     0.837875     (0.663213779445, 47)  (-0.348720913679, 84)    71182     \n",
       "19    -0.155125     (0.610853867649, 27)  (-0.350694515445, 62)    25186     \n",
       "20    -0.05725      (0.582847354665, 76)  (0.0969528036993, 84)    32195     \n",
       "21     0.77975      (0.395609756863, 21)  (-0.294433611601, 6)     30205     \n",
       "22     0.12075      (0.654557372589, 14)  (0.267240392175, 53)     53231     \n",
       "23    -0.58         (0.649822125731, 3)   (-0.224364579171, 31)    7107      \n",
       "24     0.114875     (0.6063393947, 12)    (-0.231791532171, 85)    315       \n",
       "25    -0.994375     (0.262105188039, 62)  (-0.049702837643, 50)    41490     \n",
       "26     0.77375      (0.557799525129, 46)  (0.0345942843156, 57)    80783     \n",
       "27     2.82913      (0.56139269972, 27)   (-0.483565360218, 73)    32584     \n",
       "28    -0.827375     (0.782802170349, 20)  (-0.644943066238, 78)    74530     \n",
       "29    -0.427125     (0.576356081462, -4)  (-0.446815270639, 46)    142070    \n",
       "...         ...                      ...                    ...       ...    \n",
       "18338 -0.098875     (0.655287838323, 1)   (0.843891437452, 82)     39143     \n",
       "18339 -0.534875     (0.56139269972, 44)   (-0.509956425589, -2)    23624     \n",
       "18340 -0.376125     (0.343373701784, 46)  (-0.0848270542109, 59)   224407    \n",
       "18341  0.628375     (0.515434199589, 71)  (0.138639287995, 86)     84277     \n",
       "18342 -0.456375     (0.349815744522, 42)  (-0.436492368737, 72)    37609     \n",
       "18343  0.00437499   (0.217427620754, 24)  (0.250305088733, 31)     39759     \n",
       "18344  0.835375     (0.620475044627, 17)  (-0.180458617322, 9)     116494    \n",
       "18345  0.132        (0.275841389139, 70)  (0.471491200668, 53)     842956    \n",
       "18346  0.37475      (0.410463407059, 53)  (0.471491200668, 62)     390166    \n",
       "18347  0.0315       (0.301073327678, 0)   (-0.138100526037, 34)    408381    \n",
       "18348  1.80825      (0.478552821216, -3)  (-0.238213294231, 43)    41676     \n",
       "18349 -0.348375     (0.826676956076, 1)   (-0.356312596722, 32)    21982     \n",
       "18350  0.343375     (0.626264781405, 76)  (-0.534405541576, 21)    30852     \n",
       "18351  0.13125      (0.439678120758, 25)  (0.717193198977, 13)     73686     \n",
       "18352  0.42925      (0.178857586908, 42)  (0.498397777799, 33)     109070    \n",
       "18353 -0.783        (0.535909102866, 6)   (-0.509001226895, 87)    110323    \n",
       "18354  0.10175      (0.346177147553, 88)  (-0.239579849889, 28)    111751    \n",
       "18355  0.3755       (0.198842440396, 67)  (1.39954768521, 3)       28641     \n",
       "18356 -0.432125     (0.395003400609, 7)   (0.770923045884, 39)     111239    \n",
       "18357  0.298375     (0.284298403615, 64)  (0.447881251135, 55)     98634     \n",
       "18358  0.57525      (0.166213536274, 13)  (1.68137, 5)             27394     \n",
       "18359  0.20175      (0.227462254781, 26)  (-0.0302049860684, 3)    85352     \n",
       "18360  1.21638      (0.663213779445, 83)  (-0.311504847456, 73)    364424    \n",
       "18361  0.336875     (0.201618306332, 21)  (-0.0541563218929, 42)   36904     \n",
       "18362  0.31025      (0.237149657829, 26)  (-0.179577388367, 45)    9343      \n",
       "18363  0.00600001   (0.393431381678, 71)  (1.39954768521, 53)      3811      \n",
       "18364  0.6275       (0.620475044627, 67)  (-0.582352969041, 52)    6915      \n",
       "18365 -1.47737      (0.484871250567, 47)  (-0.520256026263, 32)    20182     \n",
       "18366 -0.063        (0.919298907842, 31)  (-0.504944270707, 65)    3084      \n",
       "18367 -0.1845       (0.626264781405, 19)  (-0.470377863247, 56)    498       \n",
       "\n",
       "      ProT_0  ...   HelT_6 HelT_7 Roll_0 Roll_1 Roll_2 Roll_3 Roll_4 Roll_5  \\\n",
       "0     -2.13   ...   -1.71  -1.14  -2.13  -1.39  -2.67  -2.96  -2.28   4.65    \n",
       "1     -1.44   ...   -0.68  -2.49  -1.44  -2.41  -2.41  -2.56   5.91  -4.21    \n",
       "2     -5.08   ...   -0.74   4.64  -5.08  -1.46  -1.19  -2.48   5.91  -3.79    \n",
       "3     -2.37   ...   -3.12  -2.16  -2.37  -2.21  -1.21   3.33  -0.35  -3.61    \n",
       "4     -2.08   ...   -3.47  -0.54  -2.08  -5.74  -3.78  -3.16  -1.71   7.73    \n",
       "5     -1.61   ...   -3.38  -3.59  -1.61  -2.69  -3.27   3.17  -1.88  -2.34    \n",
       "6     -0.8    ...   -0.67  -5.1   -0.8    3.49  -0.79  -3.16  -2.7    6       \n",
       "7     -2.69   ...    5.64  -0.99  -2.69   3.95  -1.32  -2.89  -4.42  -3.61    \n",
       "8     -1.61   ...   -2.28  -4.52  -1.61   3.06  -1.61   3.98  -2.52   6.2     \n",
       "9      3.58   ...    6.73  -3.89   3.58  -2.4    4.78  -1.84  -3.63  -3.4     \n",
       "10    -4.7    ...   -5.31  -2.42  -4.7   -1.05  -0.65   2.83  -2.49  -1.52    \n",
       "11    -2.96   ...   -1.67  -3.7   -2.96   2.78  -2.52  -2.92  -3.58  -0.96    \n",
       "12    -3.7    ...   -1.66   6.98  -3.7   -1.67  -1.09  -3.46  -5.83  -1.98    \n",
       "13    -2.66   ...   -4.11   6.09  -2.66   3.58  -2.4    4.36  -2.1   -0.68    \n",
       "14    -3.01   ...   -2.54  -5.31  -3.01   7.72  -0.94  -3.13  -5.17   7.31    \n",
       "15    -2.31   ...   -2.02  -0.99  -2.31  -3.16   3.64  -0.94  -5.1   -1.57    \n",
       "16    -3.47   ...   -1.93  -2.27  -3.47  -3.96  -1.64  -2.41  -1.09   4.6     \n",
       "17    -3.17   ...   -3     -2.77  -3.17  -0.99   6.98  -1.17  -2.4   -5.74    \n",
       "18    -1.76   ...    3.93  -2.11  -1.76   2.74  -1.61  -2.83  -1.76  -1.19    \n",
       "19    -1.98   ...   -2.02  -1.73  -1.98   2.84  -2.02   5.86  -5.05  -1.36    \n",
       "20    -3.79   ...   -0.68  -3.89  -3.79  -4.03   2.79  -0.73   3.76  -1.73    \n",
       "21    -1.35   ...   -4.21   5.62  -1.35  -2.91  -2.65   4.48  -2.1   -0.68    \n",
       "22     6.83   ...    4.39  -0.74   6.83  -0.52  -5.15  -3.52  -0.98  -1.83    \n",
       "23     4.64   ...   -1.25  -2.09   4.64  -1.52  -2.33  -3.19   5.97  -3.93    \n",
       "24    -1.92   ...   -1.73  -0.68  -1.92  -3.02  -3.27   3.2   -0.7    3.76    \n",
       "25     3.71   ...   -0.74   4.39   3.71  -0.93  -3.19  -3.26   6.73  -3.47    \n",
       "26    -1.14   ...    7.73  -3.47  -1.14  -4.11   6.33  -5.95  -3.13  -0.94    \n",
       "27    -0.81   ...   -0.72  -4.63  -0.81   2.77  -2.5   -2.49  -2.27   4.64    \n",
       "28    -2.45   ...   -0.56   3.38  -2.45  -2.51  -0.78  -1.43   6.31  -3.77    \n",
       "29    -1.76   ...   -2.02   4.39  -1.76   2.63  -2.49  -1.01  -5.53  -1.01    \n",
       "...     ...   ...     ...    ...    ...    ...    ...    ...    ...    ...    \n",
       "18338 -1.9    ...   -2.58  -3.25  -1.9   -1.27  -1.5    3.74  -1.58  -1.97    \n",
       "18339 -2.33   ...   -4.21   6.31  -2.33   3.96  -3.24  -4.8   -3.58  -2.16    \n",
       "18340 -0.73   ...   -3.12  -3.81  -0.73   3.08  -2.04   5.86  -5.05  -2.28    \n",
       "18341 -1.62   ...   -2.81   6.86  -1.62   5.95  -3.05  -4.29  -5.95   7.33    \n",
       "18342 -3.51   ...   -2.08  -2.02  -3.51  -2.54   5.35   0.2   -2.82  -5.83    \n",
       "18343 -3.02   ...   -4.41  -0.86  -3.02   3.92  -1.29  -2.4    4.73  -1.12    \n",
       "18344  3.17   ...    7.33  -2.86   3.17  -3.27  -2.69  -1.33  -1.4   -4.08    \n",
       "18345 -7.53   ...   -1.83  -3.03  -7.53  -3.72  -1.52  -2.41  -1.09   4.51    \n",
       "18346 -5.85   ...   -2.77  -2.49  -5.85  -3.66  -0.78  -0.95   3.59  -3.38    \n",
       "18347 -0.66   ...   -1.25  -2.2   -0.66  -2.89   3.67  -2.04   5.8   -3.93    \n",
       "18348 -5.01   ...    6.06  -3.44  -5.01  -2.32   0.27   4.88  -4.33  -0.21    \n",
       "18349  3.85   ...    6.25   0.14   3.85  -2.18  -1.85   4.09  -4.79  -2.37    \n",
       "18350 -0.48   ...   -1.35  -2.33  -0.48  -3.7    4.23  -0.56  -1.86  -3.06    \n",
       "18351 -4.19   ...    6     -1.66  -4.19  -1.93  -2.73   4.96  -2.44  -2.64    \n",
       "18352 -6.42   ...   -1.71  -3.49  -6.42  -4.15  -3.71   5.91  -2.9    7.72    \n",
       "18353  4.23   ...    3.85  -2.1    4.23  -0.56  -1.92  -1.84  -1.86  -2.63    \n",
       "18354 -1.25   ...   -1.86   4.91  -1.25   6.17  -1.17  -2.4   -5.74  -1.98    \n",
       "18355  4.17   ...   -2.27   4.64   4.17  -1.23   4.54  -3.42  -2.82  -3.42    \n",
       "18356 -2.67   ...   -0.81   2.83  -2.67  -2.68   4.21  -5.42  -3.42  -0.78    \n",
       "18357 -3.16   ...   -2.95   6.73  -3.16  -2.67  -3.07  -2.37   5.15  -2.44    \n",
       "18358 -3.59   ...    4.35  -1.09  -3.59   7.26  -4.38  -3.48  -0.98  -1.83    \n",
       "18359 -3.75   ...   -1.35   3.9   -3.75   5.85  -3.75   4.37  -1.35  -4.67    \n",
       "18360 -4.22   ...   -0.52   6.83  -4.22  -1.67  -1.35  -3.25  -3.42  -5.05    \n",
       "18361 -2.82   ...   -6.54   3.79  -2.82   0.2    5.35  -2.43  -3.16  -4.67    \n",
       "18362 -2.83   ...   -0.52   6.06  -2.83   2.59  -0.81  -0.78  -3.46  -5.15    \n",
       "18363 -2.59   ...   -2.02  -1.73  -2.59   6.17  -3.05  -3.55  -5.74  -2.08    \n",
       "18364 -3.24   ...   -5.1   -1.01  -3.24   3.87  -2.27  -1.93   4.64  -0.86    \n",
       "18365 -3.65   ...   -2.28  -4.52  -3.65  -1.98  -1.96   2.56  -2.11   6.2     \n",
       "18366 -3.63   ...   -1.55  -2.99  -3.63  -1.98  -1.37  -1.98  -1.09   4.51    \n",
       "18367 -2.2    ...   -1.92  -0.81  -2.2   -2.06   4.91  -2.67  -1.94  -1.84    \n",
       "\n",
       "      Roll_6 Roll_7  \n",
       "0     -1.71  -1.14   \n",
       "1     -0.68  -2.49   \n",
       "2     -0.74   4.64   \n",
       "3     -3.12  -2.16   \n",
       "4     -3.47  -0.54   \n",
       "5     -3.38  -3.59   \n",
       "6     -0.67  -5.1    \n",
       "7      5.64  -0.99   \n",
       "8     -2.28  -4.52   \n",
       "9      6.73  -3.89   \n",
       "10    -5.31  -2.42   \n",
       "11    -1.67  -3.7    \n",
       "12    -1.66   6.98   \n",
       "13    -4.11   6.09   \n",
       "14    -2.54  -5.31   \n",
       "15    -2.02  -0.99   \n",
       "16    -1.93  -2.27   \n",
       "17    -3     -2.77   \n",
       "18     3.93  -2.11   \n",
       "19    -2.02  -1.73   \n",
       "20    -0.68  -3.89   \n",
       "21    -4.21   5.62   \n",
       "22     4.39  -0.74   \n",
       "23    -1.25  -2.09   \n",
       "24    -1.73  -0.68   \n",
       "25    -0.74   4.39   \n",
       "26     7.73  -3.47   \n",
       "27    -0.72  -4.63   \n",
       "28    -0.56   3.38   \n",
       "29    -2.02   4.39   \n",
       "...     ...    ...   \n",
       "18338 -2.58  -3.25   \n",
       "18339 -4.21   6.31   \n",
       "18340 -3.12  -3.81   \n",
       "18341 -2.81   6.86   \n",
       "18342 -2.08  -2.02   \n",
       "18343 -4.41  -0.86   \n",
       "18344  7.33  -2.86   \n",
       "18345 -1.83  -3.03   \n",
       "18346 -2.77  -2.49   \n",
       "18347 -1.25  -2.2    \n",
       "18348  6.06  -3.44   \n",
       "18349  6.25   0.14   \n",
       "18350 -1.35  -2.33   \n",
       "18351  6     -1.66   \n",
       "18352 -1.71  -3.49   \n",
       "18353  3.85  -2.1    \n",
       "18354 -1.86   4.91   \n",
       "18355 -2.27   4.64   \n",
       "18356 -0.81   2.83   \n",
       "18357 -2.95   6.73   \n",
       "18358  4.35  -1.09   \n",
       "18359 -1.35   3.9    \n",
       "18360 -0.52   6.83   \n",
       "18361 -6.54   3.79   \n",
       "18362 -0.52   6.06   \n",
       "18363 -2.02  -1.73   \n",
       "18364 -5.1   -1.01   \n",
       "18365 -2.28  -4.52   \n",
       "18366 -1.55  -2.99   \n",
       "18367 -1.92  -0.81   \n",
       "\n",
       "[18368 rows x 41 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_frame[all_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_feature_df_master(tf, pos, run_type=\"base_line\"):\n",
    "#     \"\"\"\n",
    "#     Given a TF and the position of the peak file of interest\n",
    "#     Creat a DataFrame with all the coordinates\n",
    "    \n",
    "#     This is the main Feature Vector\n",
    "#     \"\"\"\n",
    "#     peak_files = get_peak_files(tf)\n",
    "\n",
    "#     combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "#     E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "#     ## Calculate all the necessary features\n",
    "#     #E_score_combined = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    \n",
    "#     #Baseline model\n",
    "\n",
    "#     feature_frame = pd.DataFrame()\n",
    "\n",
    "#     test_score = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "#     double_deal = test_score.apply(pd.Series)\n",
    "#     hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    \n",
    "#     feature_frame [\"max_kmer_score\"] = double_deal[0]\n",
    "    \n",
    "#     feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    \n",
    "#     if kmers:\n",
    "#         feature_frame [\"max_kmer_score_pos\"] = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "#         feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "#     if conservation:\n",
    "#         feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#         feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "\n",
    "#     feature_frame.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     #TSS\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    \n",
    "#     #Shape features\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "        \n",
    "#     #The PWM\n",
    "#     #feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "#     return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "    tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    for j in feat_list:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_best(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_best(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in feat_list:\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(feature_frame_p.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbm_chip = []\n",
    "pbmchip2name = {}\n",
    "with open(\"/home/kipkurui/Project/Motif_Assessment/PAPER_Assessment_Data/NAR_Paper/Data/Pbm_Chip_details.txt\") as pbmnchip:\n",
    "    for line in pbmnchip:\n",
    "        if line.startswith('Tf_id'):\n",
    "            continue\n",
    "        else:\n",
    "            pbm_chip.append(line.split()[0])\n",
    "            pbmchip2name[line.split()[1]] = line.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sticky_tfs = pd.read_table(\"/home/kipkurui/Project/Others_work/Bayesian_PBM_Analysis/Bayesian_PBM_Analysis/anova_example/input/names.txt\", header=None)\n",
    "tf_list = []\n",
    "for tf in sticky_tfs[0]:\n",
    "    chip_list = glob.glob(\"/home/kipkurui/Project/MARS/Data/ChIP-seq/Derived/Posneg/%s/*\" % tf.capitalize())\n",
    "    if len(chip_list) > 0:\n",
    "        tf_list.append(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of Tfs available in PBM and ChIP with more than two peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taf1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_pbm = pd.read_table(\"/home/kipkurui/Project/Motif_Assessment/PAPER_Assessment_Data/NAR_Paper/Data/Pbm_Chip_details.txt\")\n",
    "\n",
    "in_both = tf_pbm[(tf_pbm[\"Chip_name\"] >0) == (tf_pbm[\"Pbm_name\"] >0)]\n",
    "chip_name = tf_pbm[(tf_pbm[\"Chip_name\"] >0)][\"Chip_name\"]\n",
    "chip_name = chip_name.sort_values()\n",
    "\n",
    "in_both_new = []\n",
    "for tf in chip_name:\n",
    "    if (len(get_contigmers(tf)) > 0) & (len(get_peak_files(tf)) > 1):\n",
    "        #print get_contigmers(tf)\n",
    "        in_both_new.append(tf)\n",
    "#Remove Taf1 -- Wrongly picked above\n",
    "in_both_new.pop(in_both_new.index(\"Taf1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ap2',\n",
       " 'Arid3a',\n",
       " 'Egr1',\n",
       " 'Elk1',\n",
       " 'Elk4',\n",
       " 'Ets1',\n",
       " 'Gabp',\n",
       " 'Gata3',\n",
       " 'Gr',\n",
       " 'Hnf4a',\n",
       " 'Irf3',\n",
       " 'Jund',\n",
       " 'Mafk',\n",
       " 'Max',\n",
       " 'Pou2f2',\n",
       " 'Rxra',\n",
       " 'Sp1',\n",
       " 'Srf',\n",
       " 'Tbp',\n",
       " 'Tcf7l2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_both_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_both_new2 = ['Ap2',\n",
    " 'Arid3a',\n",
    " 'Egr1',\n",
    " 'Elk1',\n",
    " 'Elk4',\n",
    " 'Ets1',\n",
    " 'Gabp',\n",
    " 'Gata3',\n",
    " 'Gr',\n",
    " 'Hnf4a',\n",
    " 'Irf3',\n",
    " 'Jund',\n",
    " 'Mafk',\n",
    " 'Max',\n",
    " 'Pou2f2',\n",
    " 'Rxra',\n",
    " 'Sp1',\n",
    " 'Srf',\n",
    " 'Tbp',\n",
    " 'Tcf7l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Same as above, but older\n",
    "\n",
    "Kept for reference purposes only, unless we find some use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to_dict = in_both.set_index(\"Chip_name\").drop(\"Tf_id\", 1)\n",
    "# pbm2chip = to_dict.to_dict()['Pbm_name']\n",
    "\n",
    "# my_new_list = []\n",
    "# for tf in pbm2chip:\n",
    "#     chip_list = glob.glob(\"/home/kipkurui/Project/MARS/Data/ChIP-seq/Downloaded/*%s*\" % pbm2chip[tf].capitalize())\n",
    "#     if len(chip_list) > 1:\n",
    "#         my_new_list.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_new_list = ['Srf','Hnf4a',\n",
    "#  'Arid3a',\n",
    "#  'Tbp',\n",
    "#  'Elk1',\n",
    "#  'Elk4',\n",
    "#  'Gata3',\n",
    "#  'Irf3',\n",
    "#   'Tcf7l2',\n",
    "#  'Pou2f2',\n",
    "#  'Egr1',\n",
    "#  'Rxra',\n",
    "#  'Ets1',\n",
    "#  'Mafk',\n",
    "#  'Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = ['max_kmer_score',\"phatsCons\",'dn_hg_score','dnase', \"tss_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_list = [\n",
    " 'max_kmer_score',\"phatsCons\",\n",
    " 'dn_hg_score2',\n",
    " 'dnase', \"tss_dist\", \"phyloP100way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for tf in in_both_new:\n",
    "#     print tf\n",
    "#     print pbmchip2name[tf]\n",
    "#     get_motif_details(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overal_list = []\n",
    "for feat in feat_list:\n",
    "    feats = feat_list[:]\n",
    "    feats.pop(feats.index(feat))\n",
    "    overal_list.append(feats[:])\n",
    "for feat1 in feat_list:\n",
    "    new_l = [feat1]\n",
    "    for feat in feat_list:\n",
    "        if not feat in new_l:\n",
    "            new_l.append(feat)\n",
    "            new_l.sort()\n",
    "            add_in = new_l[:]\n",
    "            if add_in not in overal_list:\n",
    "                overal_list.append(add_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"feature_details_importance\", \"w\") as feats:   \n",
    "    for j, yu in enumerate(overal_list):\n",
    "        #print feat_list[j]+\"_\"+str(j)\n",
    "        feats.write(\"AUC_%i\\t %s\\n\" % (j, '|'.join(yu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_feats = list(feature_frame2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pop_this(feat):\n",
    "    try:\n",
    "        all_feats.pop(all_feats.index(feat))\n",
    "    except ValueError:\n",
    "        try:\n",
    "            for i in range(8):\n",
    "                all_feats.pop(all_feats.index(feat+\"_%i\" % i))\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def pop_this(remove):\n",
    "#     for feat in list(feature_frame2.columns):\n",
    "#         if remove in feat:\n",
    "#             all_feats.pop(all_feats.index(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pop_this(\"dnase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature_frame2, trim_to = get_feature_df(tf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_best6(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    ## Calculate all the necessary features\n",
    "    #E_score_combined = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance by eliminating one, sequentially\n",
    "\n",
    "The shape features will have to be eliminated together as a group.This is an attempt to be clear on the contribution to teh accuracy by each of the features. \n",
    "\n",
    "Next, I need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repeat_tfs = [\"Gr\",\"Tbp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "    #tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_best(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_best(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in feat_list:\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance by recursive addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    feat_list = ['kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "    for j in feat_list:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs: #in_both_new:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "#         #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "#         #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        feat_list = ['kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "        loop_this = feat_list[:]\n",
    "        for i,j in enumerate(loop_this):\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            feat_list = ['kmer_score',\"phatsCons\",\n",
    "             'Roll', 'ProT', 'MGW', 'HelT',\n",
    "             'max_kmer_score_pos','dn_hg_score',\n",
    "             'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "            #print i,j\n",
    "            feat_list.pop(i)\n",
    "\n",
    "            for i in feat_list:\n",
    "                pop_this(i)\n",
    "#             print all_feats\n",
    "#         for feats in feat_list:\n",
    "#             all_feats = list(feature_frame.columns)\n",
    "#             pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we cal confidently deduce that the most informative feature as dnase and tss...however, for other features, we lose information on the quality of the model since k-mer scores are will not be confidently measured. Therefore, we eliminate the poorly performing features and then introduce. \n",
    "\n",
    "## Contribution of the DNA shape to the baseline model\n",
    "\n",
    "Here, we will have a complete feature with:\n",
    "* The max k-mer score\n",
    "* The DNase score\n",
    "* The Each of the shape features\n",
    "\n",
    "So the Idea is to start with a complete model, then one with a  variation of each of the shape features. \n",
    "\n",
    "The difficulty with these is that the model does not consider the order of the features, rather, it starts with the first ones and moves along with the rest. So, generally, the feature presented first seems to have high contribution to the tree decisions. \n",
    "\n",
    "Here we want to observe the contribution, rather than how much a dip adding the feature causes. Then decide on the contribution of each of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_df_shape(tf, pos):   \n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_shape3.txt\", \"a\") as tf_scores:\n",
    "    tf_scores.write(\"Tf_name\\tAll\\tNone\\t\")\n",
    "    for j in [ 'Roll', 'ProT', 'MGW', 'HelT']:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:#in_both_new[17:]:\n",
    "        shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_df_shape(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_df_shape(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for i in shapes:\n",
    "            pop_this(i)\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        for i,j in enumerate([ 'Roll', 'ProT', 'MGW', 'HelT']):\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']\n",
    "            #print i,j\n",
    "            shapes.pop(i)\n",
    "\n",
    "            for i in shapes:\n",
    "                pop_this(i)\n",
    "    \n",
    "#         for feats in feat_list:\n",
    "#             all_feats = list(feature_frame.columns)\n",
    "#             pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the effect of the preferred and noise k-mer scores as an additional feature\n",
    "\n",
    "Question here is to determine if it does contribute to the predictive ability of the model. Here we can extract the information from the feature contribution based on how much loss it causes. \n",
    "\n",
    "Here, we also want to use the baseline model comprosing of the DNase and kmer scores...that is after determining the best scoring function we just settle on that for any subsequent computation. The level of correlation of the various features is also informative in terms of how much more value they can add to the quality of the model. \n",
    "\n",
    "With this, the best option, is to test the performance of the baseline with k-mer model and with Dnase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_features = [[\"dnase\", \"max_kmer_score\",\"dn_hg_score\",\"dn_hg_score2\"],[\"dnase\", \"max_kmer_score\"],[\"dnase\", \"max_kmer_score\",\"dn_hg_score2\"], [\"dnase\", \"max_kmer_score\",\"dn_hg_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_noise(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_both_new[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_noise.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_noise(tf, 0)\n",
    "        #for pos in range(1,len(get_peak_files(tf)))\n",
    "        feature_frame_p,trim_to_p =  get_feature_noise(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "#         my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "#         testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "#         y_pred = my_model.predict(testdmat)\n",
    "#         tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in noise_features:\n",
    "            #all_feats = list(feature_frame.columns)\n",
    "            #pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how well the models can be generalizable\n",
    "\n",
    "To do this, I need to identify those TFS that have data in more than three cell lines, then use that infromation to test how a model trained in a rottatting manner can be used top predict binding in teh other cell lines and, what is the accuracy. Although, in a way, our current implementation where we train in one and test in another is okay, we need to check and see if there exists flactuations in performance depending on the training cell line. \n",
    "\n",
    "Given a TF with more than one cell line, we get the name and then test prediction ability of a model from one cell line in predicting performance in another cell line.\n",
    "* Start with single cell line, and if we do observe some irregularities, then --\n",
    "* Create a  model from each of the cell lines testing the perfomance in the other cell lines. \n",
    "* Determine how well we can generalize our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabp\n",
      "Gata3\n",
      "Gr\n",
      "Jund\n",
      "Mafk\n",
      "Max\n",
      "Sp1\n",
      "Srf\n",
      "Tbp\n",
      "Tcf7l2\n"
     ]
    }
   ],
   "source": [
    "over_3 = []\n",
    "for tf in in_both_new:\n",
    "    #print tf\n",
    "    peak_files = get_peak_files(tf)\n",
    "    if (len(peak_files) > 3) & (len(peak_files) < 10):\n",
    "        over_3.append(tf)\n",
    "        print tf\n",
    "    #print get_celltype(peak_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_celltype(peak_files):\n",
    "    cell_types = []\n",
    "    for i in range(len(peak_files)):\n",
    "        cell_types.append(peak_files[i].split(\"/\")[-1].split(\"Tfbs\")[-1].split(\"UniPk\")[0])\n",
    "    return cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_cell_type(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_cell_type_specificity_1.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in reapeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_cell_type(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "        for pos in range(0,len(get_peak_files(tf))-1):\n",
    "            feature_frame_p,trim_to_p =  get_feature_cell_type(tf, pos)\n",
    "\n",
    "            y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "\n",
    "            all_feats = list(feature_frame.columns)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(get_peak_files(tf))):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gabp', 'Gata3', 'Gr', 'Jund', 'Mafk', 'Max', 'Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different training cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "over_3 = ['Gabp', 'Gata3', 'Jund', 'Mafk', 'Max', 'Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "over_3 = ['Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_cell_type_specificity_recursive.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in over_3:\n",
    "        tf_scores.write(\"\\n%s\\n\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "        for train in range(len(get_peak_files(tf))):\n",
    "            pybedtools.cleanup()\n",
    "            tf_scores.write(\"%s\\t\" % get_celltype(get_peak_files(tf))[train])\n",
    "            feature_frame, trim_to = get_feature_cell_type(tf, train)\n",
    "            y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "            my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "            for pos in range(len(get_peak_files(tf))):\n",
    "                feature_frame_p,trim_to_p =  get_feature_cell_type(tf, pos)\n",
    "\n",
    "                y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "\n",
    "                all_feats = list(feature_frame.columns)\n",
    "\n",
    "                testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "                y_pred = my_model.predict(testdmat)\n",
    "\n",
    "                tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for conservation data contribution\n",
    "\n",
    "Using the same idea, we need to test the various forms of acquiring the consevation scores and their effect on the perfomance of the model. Here are to test the following:\n",
    "* Phastcons hit\n",
    "* phastcons whole site\n",
    "* Phylo hit\n",
    "* phylo whole site\n",
    "\n",
    "With each of these, we use the baseline model already defined and tested. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_conservation(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"kmer_score\"] = get_kmer_score(combined_bed, energy_score_kmer, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"phatsCons_whole\"] = apply_get_phatscon(combined_bed)\n",
    "    feature_frame[\"phyloP100way_whole\"] = apply_get_phatscon(combined_bed, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conservation_features = [[\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                  [\"dnase\", \"max_kmer_score\"],\n",
    "                  [\"dnase\", \"max_kmer_score\",\"phyloP100way\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                  [\"dnase\", \"max_kmer_score\",\"phatsCons\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                 [\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\",\"phyloP100way_whole\"],\n",
    "                 [\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\", \"phatsCons_whole\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_conservation.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tPhats_hit\\tPhylo_hit\\tPhats_wh\\tPhylo_wh\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in reapeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_conservation(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_conservation(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "#         my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "#         testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "#         y_pred = my_model.predict(testdmat)\n",
    "#         tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in conservation_features:\n",
    "            #all_feats = list(feature_frame.columns)\n",
    "            #pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly compare the performance of the k-mer and the PWM models both derived from PBM data\n",
    "\n",
    "Here, the focus is to test the best scoring function for k-mers and the PWM derived from the same data. The intention is to determine if there exists any performance difference. \n",
    "\n",
    "For the pure k-mers, I had previously tested an option of scoring using k-mer above a given threshold. The benefit of this is that it will not be confounded by the poorly scoring k-mers. However, for a given sequence, teh number of k-mers above the trheshold will shift the scores in their favour, though it can be argues that this will still help pick up sequences with high scoring k-mers.\n",
    "\n",
    "The initial testing of this did not reveal this benefit...but it will not hurt to have a more comprehensive test, as well as for the thresholds (common ones are: 0.25 or 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ap2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC 0.977606933355\n",
      "Arid3a\n",
      "AUPRC 0.880469079034\n",
      "Egr1\n",
      "AUPRC 0.910383578774\n",
      "Elk1\n",
      "AUPRC 0.97223757191\n",
      "Elk4\n",
      "AUPRC 0.960571050507\n",
      "Ets1\n",
      "AUPRC 0.943579325179\n",
      "Gabp\n",
      "AUPRC 0.952161968975\n",
      "Gata3\n",
      "AUPRC 0.907285569572\n",
      "Gr\n",
      "AUPRC 0.797471857658\n",
      "Hnf4a\n",
      "AUPRC 0.979244390734\n",
      "Irf3\n",
      "AUPRC 0.93453141081\n",
      "Jund\n",
      "AUPRC 0.911029543485\n",
      "Mafk\n",
      "AUPRC 0.963480840785\n",
      "Max\n",
      "AUPRC 0.928093808582\n",
      "Pou2f2\n",
      "AUPRC 0.945298583355\n",
      "Rxra\n",
      "AUPRC 0.711409542972\n",
      "Sp1\n",
      "AUPRC 0.908936667799\n",
      "Srf\n",
      "AUPRC 0.964137669576\n",
      "Tbp\n",
      "AUPRC 0.681431935356\n",
      "Tcf7l2\n",
      "AUPRC 0.863327983539\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_best_shape_everything.txt\", \"a\") as tf_scores:\n",
    "    with open(\"TF_feats_best_shape_everything.txt\", \"a\") as tf_feats:\n",
    "        \n",
    "        tf_scores.write(\"Tf_name\\tAUC\\tAUPRC\\t\")\n",
    "#         for i in feat_list:\n",
    "#             tf_scores.write(i+\"\\t\")\n",
    "#             tf_feats.write(i+\"\\t\")\n",
    "        for tf in in_both_new:\n",
    "            tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "            tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "            print tf\n",
    "\n",
    "            feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "            #feature_frame = shuffle_df_columns(feature_frame)\n",
    "            neg_size = trim_to\n",
    "            pos_size = trim_to\n",
    "            y_train = np.concatenate((np.ones(pos_size), np.zeros(neg_size)), axis=0)\n",
    "\n",
    "            my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "\n",
    "            feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "            \n",
    "            #shuffling the df columns to test feature importance\n",
    "            #feature_frame_p = shuffle_df_columns(feature_frame_p)\n",
    "\n",
    "\n",
    "            neg_size = trim_to_p #len(pos_bed_p)\n",
    "            pos_size = trim_to_p #len(neg_bed_p)\n",
    "\n",
    "            y_test = np.concatenate((np.ones(pos_size), np.zeros(neg_size)), axis=0)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t%s\\t\" % (roc_auc_score(y_test, y_pred),calc_auPRC(y_test, y_pred)))\n",
    "\n",
    "            #print calc_auPRC(y_test, y_pred)\n",
    "            print \"AUPRC\", roc_auc_score(y_test, y_pred)\n",
    "#             importances = my_model.get_fscore()\n",
    "#             for feat in feat_list:\n",
    "#                 pwm_score = feature_frame_p[feat]\n",
    "#                 pwm_score = np.array(pwm_score)\n",
    "#                 tf_scores.write(\"%s\\t\" % roc_auc_score(y_test, pwm_score))\n",
    "#                 tf_feats.write(\"%s\\t\" % importances[feat])\n",
    "                \n",
    "            \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_df_columns(df):\n",
    "    \"This really doesn't add much value\"\n",
    "    import random\n",
    "    asd = list(df.columns)\n",
    "    random.seed(12456)\n",
    "    random.shuffle(asd)\n",
    "    return df[asd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.967028447896\n",
    "Arid3a\n",
    "0.852579262049\n",
    "Egr1\n",
    "0.911651140092\n",
    "Elk1\n",
    "0.97114564576\n",
    "Elk4\n",
    "0.962128973571\n",
    "Ets1\n",
    "0.932960986153\n",
    "Gabp\n",
    "0.947173738305\n",
    "Gata3\n",
    "0.887761995812\n",
    "Gr\n",
    "0.743983361148\n",
    "Hnf4a\n",
    "0.965655318165\n",
    "Irf3\n",
    "0.927597905978\n",
    "Jund\n",
    "0.91214940445\n",
    "Mafk\n",
    "0.909838090472\n",
    "Max\n",
    "0.915759746733\n",
    "Pou2f2\n",
    "0.93228627328\n",
    "Rxra\n",
    "0.714333611668\n",
    "Sp1\n",
    "0.887451495053\n",
    "Srf\n",
    "0.913463596107\n",
    "Tbp\n",
    "0.650860624848\n",
    "Tcf7l2\n",
    "0.849462619421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = my_model.get_fscore()\n",
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,8), color = 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select features using threshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "thresholds = np.sort(my_model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "\t# select features using threshold\n",
    "\tselection = SelectFromModel(my_model, threshold=thresh, prefit=True)\n",
    "\tselect_X_train = selection.transform(X_train)\n",
    "\t# train model\n",
    "\tselection_model = XGBClassifier()\n",
    "\tselection_model.fit(select_X_train, y_train)\n",
    "\t# eval model\n",
    "\tselect_X_test = selection.transform(X_test)\n",
    "\ty_pred = selection_model.predict(select_X_test)\n",
    "\tpredictions = [round(value) for value in y_pred]\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?my_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_name\tAUC\tAUPRC\t\n",
    "Ap2 0.938124646676 0.932824727415\n",
    "kmer_score 0.555311297813 max_kmer_score 0.83264440124 roll_shape 0.583816820508 max_kmer_score_pos 0.735638915265 dn_hg_score 0.379098841749 dn_hg_score2 0.401589110449 dnase 0.768382748196 roll_shape 0.583816820508\n",
    "Hnf4a\n",
    "AUC 0.937684911264\n",
    "AUPRC 0.939223332641\n",
    "Arid3a\n",
    "AUC 0.860919620246\n",
    "AUPRC 0.852510683225\n",
    "Tbp\n",
    "AUC 0.770311691142\n",
    "AUPRC 0.74088930107\n",
    "Elk1\n",
    "AUC 0.948405633803\n",
    "AUPRC 0.952530567524\n",
    "Elk4\n",
    "AUC 0.935527082941\n",
    "AUPRC 0.944097868824\n",
    "Gata3\n",
    "AUC 0.868914046639\n",
    "AUPRC 0.872456804545\n",
    "Irf3\n",
    "AUC 0.913135367151\n",
    "AUPRC 0.885632142802\n",
    "Srf\n",
    "AUC 0.852947590172\n",
    "AUPRC 0.855429325735\n",
    "Tcf7l2\n",
    "AUC 0.810775240055\n",
    "AUPRC 0.819769318345\n",
    "Pou2f2\n",
    "AUC 0.884103094753\n",
    "AUPRC 0.880243202566\n",
    "Egr1\n",
    "AUC 0.853968119074\n",
    "AUPRC 0.865720123554\n",
    "Rxra\n",
    "AUC 0.691017656202\n",
    "AUPRC 0.661511909179\n",
    "Ets1\n",
    "AUC 0.866551530778\n",
    "AUPRC 0.856931625276\n",
    "Mafk\n",
    "AUC 0.902584755547\n",
    "AUPRC 0.907614209914\n",
    "Max\n",
    "AUC 0.887020126969\n",
    "AUPRC 0.876736817766\n",
    "max\n",
    "AUC 0.883304042534\n",
    "AUPRC 0.869888988934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next steps in predictions\n",
    "1. Create an additional; feature, that uses the best PWM for prediction. This should would be used during the comparison stage, to show how this model performs compared with a pwm model. We can also compare it with the pure k-mer scoring. \n",
    "2. An additional thing to validate or argue in my thesis is the value of the k-mer scoring approach being used. I need to campare the few approaches I can come across on their own. This could later be moved to chapter three or something. \n",
    "3. If time allows, how hard would it be for me to include a DNAshape feature to my model as well?\n",
    "4. When I have multiple features to work with, how can I choose te most predictive features? The number of features that would have optimal performance. \n",
    "    - Here is where recursive feature elimination may be of some use\n",
    "5. What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_frame.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max using 2000 each for all the features\n",
    "AUC 0.9342615\n",
    "AUPRC 0.924861874448\n",
    "\n",
    "max using a but the dnase feature with 4000\n",
    "AUC 0.897419625\n",
    "AUPRC 0.892977974024\n",
    "\n",
    "\n",
    "max with all but two\n",
    "AUC 0.914374375\n",
    "AUPRC 0.90579930936\n",
    "\n",
    "max for all after changing the scoing function to Energy score\n",
    "AUC 0.941812875\n",
    "AUPRC 0.93820425951\n",
    "\n",
    "\n",
    "max\n",
    "AUC 0.850881613639\n",
    "AUPRC 0.834658458345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Foxa2\n",
    "AUC 0.999999910074\n",
    "AUPRC 0.999999910089\n",
    "Gata3\n",
    "AUC 0.760884285762\n",
    "AUPRC 0.770112301698\n",
    "Max\n",
    "AUC 0.781464160056\n",
    "AUPRC 0.782942410112\n",
    "Tcf3\n",
    "AUC 0.999999994111\n",
    "AUPRC 0.999999994111\n",
    "Tcf7l2\n",
    "AUC 0.69594951989\n",
    "AUPRC 0.72373955393\n",
    "Irf3\n",
    "AUC 0.727478245484\n",
    "AUPRC 0.705951576425\n",
    "Irf4\n",
    "AUC 1.0\n",
    "AUPRC 1.0\n",
    "Hnf4a\n",
    "AUC 0.891285263782\n",
    "AUPRC 0.893551459953\n",
    "Nr2f2\n",
    "AUC 1.0\n",
    "AUPRC 1.0\n",
    "Rxra\n",
    "AUC 0.521916302461\n",
    "AUPRC 0.513172289063\n",
    "Egr1\n",
    "AUC 0.801586154663\n",
    "AUPRC 0.821077919551\n",
    "Sp4\n",
    "AUC 0.999999963477\n",
    "AUPRC 0.999999963487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tf in tf_list:\n",
    "    print tf\n",
    "    print get_contigmers(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.849744185417\n",
    "0.840914583636\n",
    "\n",
    "0.851139224626\n",
    "0.847959778538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.fit(feature_frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 3, 'min_child_weight': 1}\n",
    "\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "optimized_GBM.fit(feature_frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'subsample': [0.7, 0.8, 0.9], 'learning_rate': [0.1, 0.01]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)\n",
    "\n",
    "[mean: 0.82907, std: 0.02203, params: {'max_depth': 3, 'min_child_weight': 1},\n",
    " mean: 0.82895, std: 0.02155, params: {'max_depth': 3, 'min_child_weight': 3},\n",
    " mean: 0.82891, std: 0.02206, params: {'max_depth': 3, 'min_child_weight': 5},\n",
    " mean: 0.82663, std: 0.02266, params: {'max_depth': 5, 'min_child_weight': 1},\n",
    " mean: 0.82706, std: 0.02249, params: {'max_depth': 5, 'min_child_weight': 3},\n",
    " mean: 0.82644, std: 0.02345, params: {'max_depth': 5, 'min_child_weight': 5},\n",
    " mean: 0.82056, std: 0.02175, params: {'max_depth': 7, 'min_child_weight': 1},\n",
    " mean: 0.81939, std: 0.02062, params: {'max_depth': 7, 'min_child_weight': 3},\n",
    " mean: 0.82089, std: 0.02104, params: {'max_depth': 7, 'min_child_weight': 5}]\n",
    "\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'subsample': [0.7, 0.8, 0.9], 'learning_rate': [0.1, 0.01]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)\n",
    "\n",
    "#learning rate\n",
    "\n",
    "[mean: 0.82960, std: 0.02174, params: {'subsample': 0.7, 'learning_rate': 0.1},\n",
    " mean: 0.82907, std: 0.02203, params: {'subsample': 0.8, 'learning_rate': 0.1},\n",
    " mean: 0.82858, std: 0.02144, params: {'subsample': 0.9, 'learning_rate': 0.1},\n",
    " mean: 0.82084, std: 0.01890, params: {'subsample': 0.7, 'learning_rate': 0.01},\n",
    " mean: 0.82065, std: 0.01878, params: {'subsample': 0.8, 'learning_rate': 0.01},\n",
    " mean: 0.82001, std: 0.01869, params: {'subsample': 0.9, 'learning_rate': 0.01}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_frame.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgdmat = xgb.DMatrix(feature_frame, y_train) # Create our DMatrix to make XGBoost more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
    "# Grid Search CV optimized settings\n",
    "\n",
    "cv_xgb = xgb.cv(params = our_params, dtrain = xgdmat, num_boost_round = 3000, nfold = 5,\n",
    "                metrics = ['error'], # Make sure you enter metrics inside a list or you may encounter issues!\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#my_model = train_xgboost(feature_frame[[\"max_kmer_score\",\"dn_hg_score2\"]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score a different set of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All: 0.80158615466250405\n",
    "#kmer_pos, dnhg2: 0.76260659391564956\n",
    "\n",
    "0.78146416005633035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scikitlearn_calc_auPRC(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_auc(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.94205975000000008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_frame, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Scale teh train data \n",
    "scaler.fit(feature_frame)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "#Scale the test data as well\n",
    "scaler.fit(X_test)  # Don't cheat - fit only on training data\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_svm = clf.predict(X_test)\n",
    "print accuracy_score(y_test, pred_svm)\n",
    "print classification_report(y_test, pred_svm)\n",
    "print auc(y_test, pred_svm, reorder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print accuracy_score(y_test, pred_svm)\n",
    "print classification_report(y_test, pred_svm)\n",
    "print auc(y_test, pred_svm, reorder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print classification_report(y_pred, pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature_frame_p = pd.DataFrame(E_score_combined_p, columns=[\"kmer_score\"])\n",
    "# feature_frame_p [\"max_kmer_score\"] = max_score_combined_p\n",
    "# feature_frame_p[\"dn_hg_score\"] = dn_hg_score_combined_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the created model to predict in a new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pssm = get_jaspar_pssm(\"MA0466.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_pssm_hits(pssm, seq_file):\n",
    "    \"\"\" Predict hits in sequences using a PSSM. \"\"\"\n",
    "    from operator import itemgetter\n",
    "    import math\n",
    "    import Bio.SeqIO\n",
    "    from Bio.Alphabet import generic_dna\n",
    "    from Bio.Alphabet.IUPAC import IUPACUnambiguousDNA as unambiguousDNA\n",
    "    import tffm_module\n",
    "    from hit_module import HIT\n",
    "    hits = []\n",
    "    for record in Bio.SeqIO.parse(seq_file, \"fasta\", generic_dna):\n",
    "        record.seq.alphabet = unambiguousDNA()\n",
    "        scores = [(pos, ((score - pssm.min) / (pssm.max - pssm.min)))\n",
    "                  for pos, score in pssm.search(record.seq, pssm.min) if not\n",
    "                  math.isnan(score)]\n",
    "        if scores:\n",
    "            pos_maxi, maxi = max(scores, key=itemgetter(1))\n",
    "            strand = \"+\"\n",
    "            if pos_maxi < 0:\n",
    "                strand = \"-\"\n",
    "                pos_maxi = pos_maxi + len(record.seq)\n",
    "            hits.append(HIT(record, pos_maxi + 1, pos_maxi + pssm.length,\n",
    "                            strand, maxi))\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_learning_data(feature_frame, pos_size, neg_size):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe with the features,\n",
    "    \"\"\"\n",
    "    \n",
    "    #import pandas as pd\n",
    "    #import numpy as np\n",
    "    #from sklearn.datasets import dump_svmlight_file\n",
    "    \n",
    "    y = np.concatenate((np.ones(pos_size), np.ones(neg_size)*-1), axis=0)\n",
    "\n",
    "    target = pd.Series.from_array(y)\n",
    "    target = target.apply(int)\n",
    "    target = target.to_frame(name=\"Target\")\n",
    "    \n",
    "    target_f1 = feature_frame.T.append(target.T).T\n",
    "\n",
    "\n",
    "    cutoff = target_f1.count()[0]\n",
    "\n",
    "    ids = pd.Series.from_array(np.arange(1, cutoff+1))\n",
    "    ids = ids.apply(int)\n",
    "\n",
    "    target_f1 = target_f1.T.append(ids.to_frame(name=\"Id\").T).T\n",
    "\n",
    "\n",
    "    X = target_f1[np.setdiff1d(target_f1.columns,['Id','Target'])]\n",
    "    y = target_f1.Target\n",
    "    \n",
    "    return target_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very disappointing performaces. Why so? Is it a failure in the scorin functions, the k-mers or the model, of the choice of negative sequences? What would happen if we added more features? Which features would these be? So, what then is the next step:\n",
    "- Expand my features to include PWM\n",
    "- Use the maximum k-mer score rather than the sum\n",
    "- test a different TFs\n",
    "- make the pos and negative set equal\n",
    "- Include counts in frequency difference directly, sclalled in the same way as E-scores\n",
    "- Test other machine learning aprroaches like SVM\n",
    "- Use a k-mer model from kmerSVM and use it to score the sequences\n",
    "\n",
    "Write up on what we will have learned so far. Spend the morning implementing the above and the afternoon in the cafe, writing up on where I am so far, especially the introduction and metholology section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put any code I do not need here. Remove anything that is not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Using the Xgboost model to predict on the test data\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# # In[ ]:\n",
    "# print \"Using the model to predict on the ladder data\"\n",
    "\n",
    "# testdmat = xgb.DMatrix(hdf_scores_lad)\n",
    "# y_pred = final_gb.predict(testdmat) # Predict using our testdmat\n",
    "\n",
    "# print \"Saving the model for latter reference\"\n",
    "\n",
    "# #plot_feature_importance(final_gb, \"%s/annotations/%s/%s_features.png\" % (BASE_DIR, tfs, tfs))\n",
    "# np.savetxt(\"%s/annotations/%s/%s_xgb_v2.txt\" % (BASE_DIR, tfs,tfs), y_pred)\n",
    "\n",
    "# joblib.dump(final_gb, \"%s/annotations/%s/%s_xgboost_v2.dat\" % (BASE_DIR, tfs, tfs))\n",
    "\n",
    "\n",
    "# tau_score = pd.read_table(\"/home/kipkurui/Project/PBM_DNase/Results/PBM_Reranked/Gata3/Gata3_4964.2_deBruijn_8mers_combined.txt\")\n",
    "# tau = pd.read_table(\"/home/kipkurui/Project/Others_work/Bayesian_PBM_Analysis/Bayesian_PBM_Analysis/background_example/reweighed.txt\", header=None)\n",
    "\n",
    "# tau_score[\"E-score\"] = tau\n",
    "# tau_score.to_csv(\"rewighed_hg_dn_scores\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epeats = pd.read_table(\"/home/kipkurui/Project/PBM_DNase/Data/DNase/wgEncodeRegDnaseClusteredV3.bed\", header=None)\n",
    "repeats = pd.read_table(\"/home/kipkurui/Project/PBM_DNase/Data/DNase/wgEncodeRegDnaseClusteredV3_bed.bed\", header=None)\n",
    "repeats = pybedtools.BedTool.from_dataframe(repeats)\n",
    "#     if len(dfs) > 2000:\n",
    "#         get_top = 2000\n",
    "#     else:\n",
    "#         get_top = len(dfs)\n",
    "a = pybedtools.BedTool.from_dataframe(combined_bed)\n",
    "\n",
    "#test = a.subtract(repeats, A=True)\n",
    "#my_df = a.intersect(repeats, c=True, -f=0.5).to_dataframe()\n",
    "#return test.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trim_to_p = min(len(pos_bed_p), len(neg_bed_p))\n",
    "# #trim_to_p = 2000\n",
    "# pos_bed_p = pos_bed_p.head(trim_to_p)\n",
    "# neg_bed_p = neg_bed_p.head(trim_to_p)\n",
    "\n",
    "# combined_bed_p = pos_bed_p.append(neg_bed_p, ignore_index=True)\n",
    "\n",
    "# E_score_combined_p = get_kmer_score(combined_bed_p, energy_score_kmer, E_score_dict)\n",
    "# max_score_combined_p = get_kmer_score(combined_bed_p, max_score_kmer, E_score_dict)\n",
    "# dn_hg_score_combined2_p = get_kmer_score(combined_bed_p, max_score_kmer, dn_hg_dict2)\n",
    "# dn_hg_score_combined_p = get_kmer_score(combined_bed_p, max_score_kmer, dn_hg_dict)\n",
    "# dnase_scores_p = apply_get_max_dnase(combined_bed_p)\n",
    "\n",
    "# feature_frame_p = pd.DataFrame(E_score_combined_p, columns=[\"kmer_score\"])\n",
    "# feature_frame_p [\"max_kmer_score\"] = max_score_combined_p\n",
    "# feature_frame_p[\"dn_hg_score\"] = dn_hg_score_combined_p\n",
    "# feature_frame_p[\"dn_hg_score2\"] = dn_hg_score_combined2_p\n",
    "# feature_frame_p[\"dnase\"] = dnase_scores_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
