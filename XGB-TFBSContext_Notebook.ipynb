{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an SVM or XGBoost model\n",
    "\n",
    "The focus of this is to train a model in one dataset and try to predict in another from a fifferent cell line. My interest here is to check if I can improve our ability to classify by using the k-mer scores as follows:\n",
    " - Create a possitive and a negative set from one ChIP-seq cell line\n",
    "     - Score the sequence using:\n",
    "         - E-scores\n",
    "         - frequency difference\n",
    "         - a kmer based model generated by training the sequences / using kmer counts of all the 8-mers in each set\n",
    "     - Use the model generated to classify a given set of sequences\n",
    " \n",
    " The next question is, wha kind of codes do we need:\n",
    "     1. A scoring function for all the sequences\n",
    "     2. A quick way to get the counts of the k-mers / have a look at how some previous challenges have used this approach\n",
    "     3. A quick way to get the DNA-shape features: This will require choosing a portion of the highest scoring window to use\n",
    "     4. Test the above features first, and then see if we can expand this to other features\n",
    "     \n",
    "     \n",
    "To be able to achieve all the above, we will need to get an indepth understanding of what we are doing, the algorithm we will use, and how much we will end up doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import  exp\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pybedtools\n",
    "import pyBigWig\n",
    "import pysam\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Main SVM module and grid search function\n",
    "from sklearn import svm, grid_search\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "#For partitioning the data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "\n",
    "#Libsvm format data loading\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "#Accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, auc\n",
    "\n",
    "# Creating an learning pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from XGB_TFBSContext import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First set the path to essential, but large files\n",
    "\n",
    "### 1. The DNA Shape files\n",
    "Downloaded from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_path = \"/home/kipkurui/Dream_challenge/DNAShape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The human genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_genome = \"/home/kipkurui/Dream_challenge/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Uniformly processed ChIP-seq peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chipseq_path = \"/home/kipkurui/Project/MARS/Data/ChIP-seq/Downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BASE_DIR = \"/home/kipkurui/Dream_challenge/DreamChallenge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create kmer dictionaries for the features of interest\n",
    "We have two option here:\n",
    "1. Backround noise scalled in a simiklar maner to sticky k-mers \n",
    "1. Preferred k-mers max normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dn_hg_dict, kmer_name = get_kmer_dict_rev(\"Data/dn_hg_max_normalized.txt\", \"test\")\n",
    "\n",
    "dn_hg_dict2, kmer_name = get_kmer_dict_rev(\"Data/hg_dn_backround_noise_minmax.txt\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Score the sequences of interest\n",
    "\n",
    "#### a) K_mer score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all data prepapration steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feature_df(tf, pos):\n",
    "    \"\"\"\n",
    "    Given a TF and the position of the peak file of interest\n",
    "    Creat a DataFrame with all the coordinates\n",
    "    \n",
    "    This is the main Feature Vector\n",
    "    \"\"\"\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    ## Calculate all the necessary features\n",
    "    #E_score_combined = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "    feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "    feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete Feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_list = ['max_kmer_score','dnase','kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf = \"Max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pop_this(feat):\n",
    "    try:\n",
    "        all_feats.pop(all_feats.index(feat))\n",
    "    except ValueError:\n",
    "        try:\n",
    "            for i in range(8):\n",
    "                all_feats.pop(all_feats.index(feat+\"_%i\" % i))\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train a model using the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open(\"Results/TF_scores_feature_importance_recursive_pop.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "#     tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "#     for j in feat_list:\n",
    "#         tf_scores.write(\"%s\\t\" % j)\n",
    "#     for tf in [\"Max\"]:\n",
    "#         tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "#         #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "#         print tf\n",
    "#         pybedtools.cleanup()\n",
    "        \n",
    "#         feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "#         feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "#         y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "#         y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "#         all_feats = list(feature_frame.columns)\n",
    "        \n",
    "#         #All\n",
    "#         my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "#         testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "#         y_pred = my_model.predict(testdmat)\n",
    "#         tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "#         for feats in feat_list:\n",
    "#             all_feats = list(feature_frame.columns)\n",
    "#             pop_this(feats)\n",
    "#             my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "#             testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "#             y_pred = my_model.predict(testdmat)\n",
    "\n",
    "#             tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_df_master(tf, pos, run_type=\"base_line\"):\n",
    "    \"\"\"\n",
    "    Given a TF and the position of the peak file of interest\n",
    "    Creat a DataFrame with all the coordinates\n",
    "    \n",
    "    This is the main Feature Vector\n",
    "    \"\"\"\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    ## Calculate all the necessary features\n",
    "    #E_score_combined = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    \n",
    "    #Baseline model\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    \n",
    "    feature_frame [\"max_kmer_score\"] = double_deal[0]\n",
    "    \n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    \n",
    "    if kmers:\n",
    "        feature_frame [\"max_kmer_score_pos\"] = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "        feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    if conservation:\n",
    "        feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "        feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #TSS\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    \n",
    "    #Shape features\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "        \n",
    "    #The PWM\n",
    "    #feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "    tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    for j in feat_list:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_best(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_best(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in feat_list:\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(feature_frame_p.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbm_chip = []\n",
    "pbmchip2name = {}\n",
    "with open(\"Data/Pbm_Chip_details.txt\") as pbmnchip:\n",
    "    for line in pbmnchip:\n",
    "        if line.startswith('Tf_id'):\n",
    "            continue\n",
    "        else:\n",
    "            pbm_chip.append(line.split()[0])\n",
    "            pbmchip2name[line.split()[1]] = line.split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of TFs affected by Sticky k-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sticky_tfs = pd.read_table(\"Data/names.txt\", header=None)\n",
    "tf_list = []\n",
    "for tf in sticky_tfs[0]:\n",
    "    chip_list = glob.glob(\"/home/kipkurui/Project/MARS/Data/ChIP-seq/Derived/Posneg/%s/*\" % tf.capitalize())\n",
    "    if len(chip_list) > 0:\n",
    "        tf_list.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_list = ['Foxa2',\n",
    " 'Gata3',\n",
    " 'Max',\n",
    " 'Tcf3',\n",
    " 'Tcf7l2',\n",
    " 'Irf3',\n",
    " 'Irf4',\n",
    " 'Hnf4a',\n",
    " 'Nr2f2',\n",
    " 'Rxra',\n",
    " 'Egr1',\n",
    " 'Sp4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of Tfs available in PBM and ChIP with more than two peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_contigmers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6a5e8c10ae87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0min_both_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchip_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_contigmers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_peak_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print get_contigmers(tf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0min_both_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_contigmers' is not defined"
     ]
    }
   ],
   "source": [
    "tf_pbm = pd.read_table(\"Data/Pbm_Chip_details.txt\")\n",
    "\n",
    "in_both = tf_pbm[(tf_pbm[\"Chip_name\"] >0) == (tf_pbm[\"Pbm_name\"] >0)]\n",
    "chip_name = tf_pbm[(tf_pbm[\"Chip_name\"] >0)][\"Chip_name\"]\n",
    "chip_name = chip_name.sort_values()\n",
    "\n",
    "in_both_new = []\n",
    "for tf in chip_name:\n",
    "    if (len(get_contigmers(tf)) > 0) & (len(get_peak_files(tf)) > 1):\n",
    "        #print get_contigmers(tf)\n",
    "        in_both_new.append(tf)\n",
    "#Remove Taf1 -- Wrongly picked above\n",
    "in_both_new.pop(in_both_new.index(\"Taf1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ap2',\n",
       " 'Arid3a',\n",
       " 'Egr1',\n",
       " 'Elk1',\n",
       " 'Elk4',\n",
       " 'Ets1',\n",
       " 'Gabp',\n",
       " 'Gata3',\n",
       " 'Gr',\n",
       " 'Hnf4a',\n",
       " 'Irf3',\n",
       " 'Jund',\n",
       " 'Mafk',\n",
       " 'Max',\n",
       " 'Pou2f2',\n",
       " 'Rxra',\n",
       " 'Sp1',\n",
       " 'Srf',\n",
       " 'Tbp',\n",
       " 'Tcf7l2']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_both_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_both_new2 = ['Ap2',\n",
    " 'Arid3a',\n",
    " 'Egr1',\n",
    " 'Elk1',\n",
    " 'Elk4',\n",
    " 'Ets1',\n",
    " 'Gabp',\n",
    " 'Gata3',\n",
    " 'Gr',\n",
    " 'Hnf4a',\n",
    " 'Irf3',\n",
    " 'Jund',\n",
    " 'Mafk',\n",
    " 'Max',\n",
    " 'Pou2f2',\n",
    " 'Rxra',\n",
    " 'Sp1',\n",
    " 'Srf',\n",
    " 'Tbp',\n",
    " 'Tcf7l2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Same as above, but older\n",
    "\n",
    "Kept for reference purposes only, unless we find some use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to_dict = in_both.set_index(\"Chip_name\").drop(\"Tf_id\", 1)\n",
    "# pbm2chip = to_dict.to_dict()['Pbm_name']\n",
    "\n",
    "# my_new_list = []\n",
    "# for tf in pbm2chip:\n",
    "#     chip_list = glob.glob(\"/home/kipkurui/Project/MARS/Data/ChIP-seq/Downloaded/*%s*\" % pbm2chip[tf].capitalize())\n",
    "#     if len(chip_list) > 1:\n",
    "#         my_new_list.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_new_list = ['Srf','Hnf4a',\n",
    "#  'Arid3a',\n",
    "#  'Tbp',\n",
    "#  'Elk1',\n",
    "#  'Elk4',\n",
    "#  'Gata3',\n",
    "#  'Irf3',\n",
    "#   'Tcf7l2',\n",
    "#  'Pou2f2',\n",
    "#  'Egr1',\n",
    "#  'Rxra',\n",
    "#  'Ets1',\n",
    "#  'Mafk',\n",
    "#  'Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = ['max_kmer_score',\"phatsCons\",'dn_hg_score','dnase', \"tss_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_list = [\n",
    " 'max_kmer_score',\"phatsCons\",\n",
    " 'dn_hg_score2',\n",
    " 'dnase', \"tss_dist\", \"phyloP100way\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for tf in in_both_new:\n",
    "#     print tf\n",
    "#     print pbmchip2name[tf]\n",
    "#     get_motif_details(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overal_list = []\n",
    "for feat in feat_list:\n",
    "    feats = feat_list[:]\n",
    "    feats.pop(feats.index(feat))\n",
    "    overal_list.append(feats[:])\n",
    "for feat1 in feat_list:\n",
    "    new_l = [feat1]\n",
    "    for feat in feat_list:\n",
    "        if not feat in new_l:\n",
    "            new_l.append(feat)\n",
    "            new_l.sort()\n",
    "            add_in = new_l[:]\n",
    "            if add_in not in overal_list:\n",
    "                overal_list.append(add_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"feature_details_importance\", \"w\") as feats:   \n",
    "    for j, yu in enumerate(overal_list):\n",
    "        #print feat_list[j]+\"_\"+str(j)\n",
    "        feats.write(\"AUC_%i\\t %s\\n\" % (j, '|'.join(yu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_feats = list(feature_frame2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def pop_this(remove):\n",
    "#     for feat in list(feature_frame2.columns):\n",
    "#         if remove in feat:\n",
    "#             all_feats.pop(all_feats.index(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pop_this(\"dnase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature_frame2, trim_to = get_feature_df(tf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_best6(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    ## Calculate all the necessary features\n",
    "    #E_score_combined = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance by eliminating one, sequentially\n",
    "\n",
    "The shape features will have to be eliminated together as a group.This is an attempt to be clear on the contribution to teh accuracy by each of the features. \n",
    "\n",
    "Next, I need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repeat_tfs = [\"Gr\",\"Tbp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    \n",
    "    #tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_best(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_best(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in feat_list:\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance by recursive addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_all.txt\", \"a\") as tf_scores:\n",
    "    tf_scores.write(\"Tf_name\\tAll\\t\")\n",
    "    feat_list = ['kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "    for j in feat_list:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs: #in_both_new:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "#         #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "#         #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        feat_list = ['kmer_score',\"phatsCons\",\n",
    " 'Roll', 'ProT', 'MGW', 'HelT',\n",
    " 'max_kmer_score_pos','dn_hg_score',\n",
    " 'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "        loop_this = feat_list[:]\n",
    "        for i,j in enumerate(loop_this):\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            feat_list = ['kmer_score',\"phatsCons\",\n",
    "             'Roll', 'ProT', 'MGW', 'HelT',\n",
    "             'max_kmer_score_pos','dn_hg_score',\n",
    "             'dn_hg_score2',\"tss_dist\", \"phyloP100way\"]\n",
    "            #print i,j\n",
    "            feat_list.pop(i)\n",
    "\n",
    "            for i in feat_list:\n",
    "                pop_this(i)\n",
    "#             print all_feats\n",
    "#         for feats in feat_list:\n",
    "#             all_feats = list(feature_frame.columns)\n",
    "#             pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we cal confidently deduce that the most informative feature as dnase and tss...however, for other features, we lose information on the quality of the model since k-mer scores are will not be confidently measured. Therefore, we eliminate the poorly performing features and then introduce. \n",
    "\n",
    "## Contribution of the DNA shape to the baseline model\n",
    "\n",
    "Here, we will have a complete feature with:\n",
    "* The max k-mer score\n",
    "* The DNase score\n",
    "* The Each of the shape features\n",
    "\n",
    "So the Idea is to start with a complete model, then one with a  variation of each of the shape features. \n",
    "\n",
    "The difficulty with these is that the model does not consider the order of the features, rather, it starts with the first ones and moves along with the rest. So, generally, the feature presented first seems to have high contribution to the tree decisions. \n",
    "\n",
    "Here we want to observe the contribution, rather than how much a dip adding the feature causes. Then decide on the contribution of each of the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_df_shape(tf, pos):   \n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_shape3.txt\", \"a\") as tf_scores:\n",
    "    tf_scores.write(\"Tf_name\\tAll\\tNone\\t\")\n",
    "    for j in [ 'Roll', 'ProT', 'MGW', 'HelT']:\n",
    "        tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:#in_both_new[17:]:\n",
    "        shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_df_shape(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_df_shape(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for i in shapes:\n",
    "            pop_this(i)\n",
    "        my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "        testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "        y_pred = my_model.predict(testdmat)\n",
    "        tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        for i,j in enumerate([ 'Roll', 'ProT', 'MGW', 'HelT']):\n",
    "            all_feats = list(feature_frame.columns)\n",
    "            shapes = [ 'Roll', 'ProT', 'MGW', 'HelT']\n",
    "            #print i,j\n",
    "            shapes.pop(i)\n",
    "\n",
    "            for i in shapes:\n",
    "                pop_this(i)\n",
    "    \n",
    "#         for feats in feat_list:\n",
    "#             all_feats = list(feature_frame.columns)\n",
    "#             pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the effect of the preferred and noise k-mer scores as an additional feature\n",
    "\n",
    "Question here is to determine if it does contribute to the predictive ability of the model. Here we can extract the information from the feature contribution based on how much loss it causes. \n",
    "\n",
    "Here, we also want to use the baseline model comprosing of the DNase and kmer scores...that is after determining the best scoring function we just settle on that for any subsequent computation. The level of correlation of the various features is also informative in terms of how much more value they can add to the quality of the model. \n",
    "\n",
    "With this, the best option, is to test the performance of the baseline with k-mer model and with Dnase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_features = [[\"dnase\", \"max_kmer_score\",\"dn_hg_score\",\"dn_hg_score2\"],[\"dnase\", \"max_kmer_score\"],[\"dnase\", \"max_kmer_score\",\"dn_hg_score2\"], [\"dnase\", \"max_kmer_score\",\"dn_hg_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_noise(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "#     feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "#     feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_both_new[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_noise.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in repeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_noise(tf, 0)\n",
    "        #for pos in range(1,len(get_peak_files(tf)))\n",
    "        feature_frame_p,trim_to_p =  get_feature_noise(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "#         my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "#         testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "#         y_pred = my_model.predict(testdmat)\n",
    "#         tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in noise_features:\n",
    "            #all_feats = list(feature_frame.columns)\n",
    "            #pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test how well the models can be generalizable\n",
    "\n",
    "To do this, I need to identify those TFS that have data in more than three cell lines, then use that infromation to test how a model trained in a rottatting manner can be used top predict binding in teh other cell lines and, what is the accuracy. Although, in a way, our current implementation where we train in one and test in another is okay, we need to check and see if there exists flactuations in performance depending on the training cell line. \n",
    "\n",
    "Given a TF with more than one cell line, we get the name and then test prediction ability of a model from one cell line in predicting performance in another cell line.\n",
    "* Start with single cell line, and if we do observe some irregularities, then --\n",
    "* Create a  model from each of the cell lines testing the perfomance in the other cell lines. \n",
    "* Determine how well we can generalize our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabp\n",
      "Gata3\n",
      "Gr\n",
      "Jund\n",
      "Mafk\n",
      "Max\n",
      "Sp1\n",
      "Srf\n",
      "Tbp\n",
      "Tcf7l2\n"
     ]
    }
   ],
   "source": [
    "over_3 = []\n",
    "for tf in in_both_new:\n",
    "    #print tf\n",
    "    peak_files = get_peak_files(tf)\n",
    "    if (len(peak_files) > 3) & (len(peak_files) < 10):\n",
    "        over_3.append(tf)\n",
    "        print tf\n",
    "    #print get_celltype(peak_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_celltype(peak_files):\n",
    "    cell_types = []\n",
    "    for i in range(len(peak_files)):\n",
    "        cell_types.append(peak_files[i].split(\"/\")[-1].split(\"Tfbs\")[-1].split(\"UniPk\")[0])\n",
    "    return cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_cell_type(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "    feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "    pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "    neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "    pos_neg_tss = pos_tss.append(neg_tss)\n",
    "    pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "    feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "    for shape in \"ProT MGW HelT Roll\".split():\n",
    "        #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "        feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "        feature_fr.columns = get_shape_names(shape)\n",
    "        feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_cell_type_specificity_1.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in reapeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_cell_type(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "        for pos in range(0,len(get_peak_files(tf))-1):\n",
    "            feature_frame_p,trim_to_p =  get_feature_cell_type(tf, pos)\n",
    "\n",
    "            y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "\n",
    "            all_feats = list(feature_frame.columns)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(get_peak_files(tf))):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gabp', 'Gata3', 'Gr', 'Jund', 'Mafk', 'Max', 'Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the different training cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "over_3 = ['Gabp', 'Gata3', 'Jund', 'Mafk', 'Max', 'Sp1', 'Srf', 'Tbp', 'Tcf7l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "over_3 = ['Max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_cell_type_specificity_recursive.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tNoise\\tPreferred\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in over_3:\n",
    "        tf_scores.write(\"\\n%s\\n\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "        for train in range(len(get_peak_files(tf))):\n",
    "            pybedtools.cleanup()\n",
    "            tf_scores.write(\"%s\\t\" % get_celltype(get_peak_files(tf))[train])\n",
    "            feature_frame, trim_to = get_feature_cell_type(tf, train)\n",
    "            y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "            my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "            for pos in range(len(get_peak_files(tf))):\n",
    "                feature_frame_p,trim_to_p =  get_feature_cell_type(tf, pos)\n",
    "\n",
    "                y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "\n",
    "                all_feats = list(feature_frame.columns)\n",
    "\n",
    "                testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "                y_pred = my_model.predict(testdmat)\n",
    "\n",
    "                tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for conservation data contribution\n",
    "\n",
    "Using the same idea, we need to test the various forms of acquiring the consevation scores and their effect on the perfomance of the model. Here are to test the following:\n",
    "* Phastcons hit\n",
    "* phastcons whole site\n",
    "* Phylo hit\n",
    "* phylo whole site\n",
    "\n",
    "With each of these, we use the baseline model already defined and tested. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_conservation(tf, pos):\n",
    "    peak_files = get_peak_files(tf)\n",
    "\n",
    "    combined_bed, trim_to = get_combined_bed(peak_files[pos])\n",
    "\n",
    "    E_score_dict, kmer_name = get_contigmers_dict(get_contigmers(tf)[0],\"test\")\n",
    "\n",
    "    feature_frame = pd.DataFrame()\n",
    "#     feature_frame[\"sum_kmer_score\"] = get_kmer_score(combined_bed, sum_kmer_score, E_score_dict)\n",
    "    #feature_frame[\"dnase\"] = apply_get_max_dnase(combined_bed)\n",
    "    feature_frame [\"max_kmer_score\"] = get_kmer_score(combined_bed, max_score_kmer, E_score_dict)\n",
    "    test_score = get_kmer_score(combined_bed, max_score_kmer_pos, E_score_dict)\n",
    "    double_deal = test_score.apply(pd.Series)\n",
    "#     feature_frame [\"max_kmer_score_pos\"] = double_deal[0]\n",
    "    hits_df = get_hits_df(double_deal, combined_bed)\n",
    "    feature_frame[\"dnase\"] = apply_get_max_dnase(hits_df)\n",
    "    feature_frame[\"phatsCons\"] = apply_get_phatscon(hits_df)\n",
    "    feature_frame[\"phyloP100way\"] = apply_get_phatscon(hits_df, \"phyloP100way\")\n",
    "    \n",
    "    feature_frame[\"phatsCons_whole\"] = apply_get_phatscon(combined_bed)\n",
    "    feature_frame[\"phyloP100way_whole\"] = apply_get_phatscon(combined_bed, \"phyloP100way\")\n",
    "    \n",
    "#     feature_frame[\"dn_hg_score\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict)\n",
    "#     feature_frame[\"dn_hg_score2\"] = get_kmer_score(combined_bed, max_score_kmer, dn_hg_dict2)\n",
    "#     feature_frame[\"pwm_score\"] = get_kmer_score(combined_bed, energyscore, get_motif_details(tf))\n",
    "    feature_frame.reset_index(drop=True, inplace=True)\n",
    "#     pos_tss = get_distance_to_tss(hits_df.head(trim_to))\n",
    "#     neg_tss = get_distance_to_tss(hits_df.tail(trim_to))\n",
    "#     pos_neg_tss = pos_tss.append(neg_tss)\n",
    "#     pos_neg_tss.reset_index(drop=True, inplace=True) \n",
    "#     feature_frame[\"tss_dist\"] = pos_neg_tss\n",
    "#     for shape in \"ProT MGW HelT Roll\".split():\n",
    "#         #feature_frame[\"%s_shape\" % shape] = apply_get_shape(hits_df, shape)\n",
    "#         feature_fr = apply_get_full_shape(hits_df).apply(pd.Series)\n",
    "#         feature_fr.columns = get_shape_names(shape)\n",
    "#         feature_frame = feature_frame.T.append(feature_fr.T).T\n",
    "    return feature_frame, trim_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conservation_features = [[\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                  [\"dnase\", \"max_kmer_score\"],\n",
    "                  [\"dnase\", \"max_kmer_score\",\"phyloP100way\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                  [\"dnase\", \"max_kmer_score\",\"phatsCons\", \"phatsCons_whole\",\"phyloP100way_whole\"],\n",
    "                 [\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\",\"phyloP100way_whole\"],\n",
    "                 [\"dnase\", \"max_kmer_score\",\"phatsCons\",\"phyloP100way\", \"phatsCons_whole\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tbp\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_feature_importance_recursive_conservation.txt\", \"a\") as tf_scores:\n",
    "    #tf_scores.write(\"Tf_name\\tAll\\tNone\\tPhats_hit\\tPhylo_hit\\tPhats_wh\\tPhylo_wh\\t\")\n",
    "    #for j in feat_list:\n",
    "        #tf_scores.write(\"%s\\t\" % j)\n",
    "    for tf in reapeat_tfs:\n",
    "        tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "        #tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "        print tf\n",
    "\n",
    "        feature_frame, trim_to = get_feature_conservation(tf, 0)\n",
    "        feature_frame_p,trim_to_p =  get_feature_conservation(tf, -1)\n",
    "        y_train = np.concatenate((np.ones(trim_to), np.zeros(trim_to)), axis=0)\n",
    "        y_test = np.concatenate((np.ones(trim_to_p), np.zeros(trim_to_p)), axis=0)\n",
    "        \n",
    "        all_feats = list(feature_frame.columns)\n",
    "        \n",
    "        #All\n",
    "#         my_model = train_xgboost(feature_frame[all_feats], y_train, tf)\n",
    "#         testdmat = xgb.DMatrix(feature_frame_p[all_feats], y_test)\n",
    "#         y_pred = my_model.predict(testdmat)\n",
    "#         tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))\n",
    "        \n",
    "        for feats in conservation_features:\n",
    "            #all_feats = list(feature_frame.columns)\n",
    "            #pop_this(feats)\n",
    "            my_model = train_xgboost(feature_frame[feats], y_train, tf)\n",
    "            \n",
    "            testdmat = xgb.DMatrix(feature_frame_p[feats], y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t\" % (roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly compare the performance of the k-mer and the PWM models both derived from PBM data\n",
    "\n",
    "Here, the focus is to test the best scoring function for k-mers and the PWM derived from the same data. The intention is to determine if there exists any performance difference. \n",
    "\n",
    "For the pure k-mers, I had previously tested an option of scoring using k-mer above a given threshold. The benefit of this is that it will not be confounded by the poorly scoring k-mers. However, for a given sequence, teh number of k-mers above the trheshold will shift the scores in their favour, though it can be argues that this will still help pick up sequences with high scoring k-mers.\n",
    "\n",
    "The initial testing of this did not reveal this benefit...but it will not hurt to have a more comprehensive test, as well as for the thresholds (common ones are: 0.25 or 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ap2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kipkurui/anaconda2/envs/dream_challenge/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUPRC 0.977606933355\n",
      "Arid3a\n",
      "AUPRC 0.880469079034\n",
      "Egr1\n",
      "AUPRC 0.910383578774\n",
      "Elk1\n",
      "AUPRC 0.97223757191\n",
      "Elk4\n",
      "AUPRC 0.960571050507\n",
      "Ets1\n",
      "AUPRC 0.943579325179\n",
      "Gabp\n",
      "AUPRC 0.952161968975\n",
      "Gata3\n",
      "AUPRC 0.907285569572\n",
      "Gr\n",
      "AUPRC 0.797471857658\n",
      "Hnf4a\n",
      "AUPRC 0.979244390734\n",
      "Irf3\n",
      "AUPRC 0.93453141081\n",
      "Jund\n",
      "AUPRC 0.911029543485\n",
      "Mafk\n",
      "AUPRC 0.963480840785\n",
      "Max\n",
      "AUPRC 0.928093808582\n",
      "Pou2f2\n",
      "AUPRC 0.945298583355\n",
      "Rxra\n",
      "AUPRC 0.711409542972\n",
      "Sp1\n",
      "AUPRC 0.908936667799\n",
      "Srf\n",
      "AUPRC 0.964137669576\n",
      "Tbp\n",
      "AUPRC 0.681431935356\n",
      "Tcf7l2\n",
      "AUPRC 0.863327983539\n"
     ]
    }
   ],
   "source": [
    "with open(\"TF_scores_best_shape_everything.txt\", \"a\") as tf_scores:\n",
    "    with open(\"TF_feats_best_shape_everything.txt\", \"a\") as tf_feats:\n",
    "        \n",
    "        tf_scores.write(\"Tf_name\\tAUC\\tAUPRC\\t\")\n",
    "#         for i in feat_list:\n",
    "#             tf_scores.write(i+\"\\t\")\n",
    "#             tf_feats.write(i+\"\\t\")\n",
    "        for tf in in_both_new:\n",
    "            tf_scores.write(\"\\n%s\\t\" % tf)\n",
    "            tf_feats.write(\"\\n%s\\t\" % tf)\n",
    "            print tf\n",
    "\n",
    "            feature_frame, trim_to = get_feature_df(tf, 0)\n",
    "            #feature_frame = shuffle_df_columns(feature_frame)\n",
    "            neg_size = trim_to\n",
    "            pos_size = trim_to\n",
    "            y_train = np.concatenate((np.ones(pos_size), np.zeros(neg_size)), axis=0)\n",
    "\n",
    "            my_model = train_xgboost(feature_frame, y_train, tf)\n",
    "\n",
    "            feature_frame_p,trim_to_p =  get_feature_df(tf, -1)\n",
    "            \n",
    "            #shuffling the df columns to test feature importance\n",
    "            #feature_frame_p = shuffle_df_columns(feature_frame_p)\n",
    "\n",
    "\n",
    "            neg_size = trim_to_p #len(pos_bed_p)\n",
    "            pos_size = trim_to_p #len(neg_bed_p)\n",
    "\n",
    "            y_test = np.concatenate((np.ones(pos_size), np.zeros(neg_size)), axis=0)\n",
    "\n",
    "            testdmat = xgb.DMatrix(feature_frame_p, y_test)\n",
    "\n",
    "            y_pred = my_model.predict(testdmat)\n",
    "\n",
    "            tf_scores.write(\"%s\\t%s\\t\" % (roc_auc_score(y_test, y_pred),calc_auPRC(y_test, y_pred)))\n",
    "\n",
    "            #print calc_auPRC(y_test, y_pred)\n",
    "            print \"AUPRC\", roc_auc_score(y_test, y_pred)\n",
    "#             importances = my_model.get_fscore()\n",
    "#             for feat in feat_list:\n",
    "#                 pwm_score = feature_frame_p[feat]\n",
    "#                 pwm_score = np.array(pwm_score)\n",
    "#                 tf_scores.write(\"%s\\t\" % roc_auc_score(y_test, pwm_score))\n",
    "#                 tf_feats.write(\"%s\\t\" % importances[feat])\n",
    "                \n",
    "            \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pybedtools.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_df_columns(df):\n",
    "    \"This really doesn't add much value\"\n",
    "    import random\n",
    "    asd = list(df.columns)\n",
    "    random.seed(12456)\n",
    "    random.shuffle(asd)\n",
    "    return df[asd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.967028447896\n",
    "Arid3a\n",
    "0.852579262049\n",
    "Egr1\n",
    "0.911651140092\n",
    "Elk1\n",
    "0.97114564576\n",
    "Elk4\n",
    "0.962128973571\n",
    "Ets1\n",
    "0.932960986153\n",
    "Gabp\n",
    "0.947173738305\n",
    "Gata3\n",
    "0.887761995812\n",
    "Gr\n",
    "0.743983361148\n",
    "Hnf4a\n",
    "0.965655318165\n",
    "Irf3\n",
    "0.927597905978\n",
    "Jund\n",
    "0.91214940445\n",
    "Mafk\n",
    "0.909838090472\n",
    "Max\n",
    "0.915759746733\n",
    "Pou2f2\n",
    "0.93228627328\n",
    "Rxra\n",
    "0.714333611668\n",
    "Sp1\n",
    "0.887451495053\n",
    "Srf\n",
    "0.913463596107\n",
    "Tbp\n",
    "0.650860624848\n",
    "Tcf7l2\n",
    "0.849462619421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = my_model.get_fscore()\n",
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,8), color = 'orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select features using threshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "thresholds = np.sort(my_model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "\t# select features using threshold\n",
    "\tselection = SelectFromModel(my_model, threshold=thresh, prefit=True)\n",
    "\tselect_X_train = selection.transform(X_train)\n",
    "\t# train model\n",
    "\tselection_model = XGBClassifier()\n",
    "\tselection_model.fit(select_X_train, y_train)\n",
    "\t# eval model\n",
    "\tselect_X_test = selection.transform(X_test)\n",
    "\ty_pred = selection_model.predict(select_X_test)\n",
    "\tpredictions = [round(value) for value in y_pred]\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next steps in predictions\n",
    "1. Create an additional; feature, that uses the best PWM for prediction. This should would be used during the comparison stage, to show how this model performs compared with a pwm model. We can also compare it with the pure k-mer scoring. \n",
    "2. An additional thing to validate or argue in my thesis is the value of the k-mer scoring approach being used. I need to campare the few approaches I can come across on their own. This could later be moved to chapter three or something. \n",
    "3. If time allows, how hard would it be for me to include a DNAshape feature to my model as well?\n",
    "4. When I have multiple features to work with, how can I choose te most predictive features? The number of features that would have optimal performance. \n",
    "    - Here is where recursive feature elimination may be of some use\n",
    "5. What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_frame.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max using 2000 each for all the features\n",
    "AUC 0.9342615\n",
    "AUPRC 0.924861874448\n",
    "\n",
    "max using a but the dnase feature with 4000\n",
    "AUC 0.897419625\n",
    "AUPRC 0.892977974024\n",
    "\n",
    "\n",
    "max with all but two\n",
    "AUC 0.914374375\n",
    "AUPRC 0.90579930936\n",
    "\n",
    "max for all after changing the scoing function to Energy score\n",
    "AUC 0.941812875\n",
    "AUPRC 0.93820425951\n",
    "\n",
    "\n",
    "max\n",
    "AUC 0.850881613639\n",
    "AUPRC 0.834658458345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tf in tf_list:\n",
    "    print tf\n",
    "    print get_contigmers(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.fit(feature_frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\n",
    "ind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth': 3, 'min_child_weight': 1}\n",
    "\n",
    "\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "optimized_GBM.fit(feature_frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'subsample': [0.7, 0.8, 0.9], 'learning_rate': [0.1, 0.01]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)\n",
    "\n",
    "[mean: 0.82907, std: 0.02203, params: {'max_depth': 3, 'min_child_weight': 1},\n",
    " mean: 0.82895, std: 0.02155, params: {'max_depth': 3, 'min_child_weight': 3},\n",
    " mean: 0.82891, std: 0.02206, params: {'max_depth': 3, 'min_child_weight': 5},\n",
    " mean: 0.82663, std: 0.02266, params: {'max_depth': 5, 'min_child_weight': 1},\n",
    " mean: 0.82706, std: 0.02249, params: {'max_depth': 5, 'min_child_weight': 3},\n",
    " mean: 0.82644, std: 0.02345, params: {'max_depth': 5, 'min_child_weight': 5},\n",
    " mean: 0.82056, std: 0.02175, params: {'max_depth': 7, 'min_child_weight': 1},\n",
    " mean: 0.81939, std: 0.02062, params: {'max_depth': 7, 'min_child_weight': 3},\n",
    " mean: 0.82089, std: 0.02104, params: {'max_depth': 7, 'min_child_weight': 5}]\n",
    "\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
    "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
    "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
    "       fit_params={}, iid=True, n_jobs=-1,\n",
    "       param_grid={'subsample': [0.7, 0.8, 0.9], 'learning_rate': [0.1, 0.01]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)\n",
    "\n",
    "#learning rate\n",
    "\n",
    "[mean: 0.82960, std: 0.02174, params: {'subsample': 0.7, 'learning_rate': 0.1},\n",
    " mean: 0.82907, std: 0.02203, params: {'subsample': 0.8, 'learning_rate': 0.1},\n",
    " mean: 0.82858, std: 0.02144, params: {'subsample': 0.9, 'learning_rate': 0.1},\n",
    " mean: 0.82084, std: 0.01890, params: {'subsample': 0.7, 'learning_rate': 0.01},\n",
    " mean: 0.82065, std: 0.01878, params: {'subsample': 0.8, 'learning_rate': 0.01},\n",
    " mean: 0.82001, std: 0.01869, params: {'subsample': 0.9, 'learning_rate': 0.01}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_GBM.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_frame.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgdmat = xgb.DMatrix(feature_frame, y_train) # Create our DMatrix to make XGBoost more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "our_params = {'eta': 0.1, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic', 'max_depth':3, 'min_child_weight':1} \n",
    "# Grid Search CV optimized settings\n",
    "\n",
    "cv_xgb = xgb.cv(params = our_params, dtrain = xgdmat, num_boost_round = 3000, nfold = 5,\n",
    "                metrics = ['error'], # Make sure you enter metrics inside a list or you may encounter issues!\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_xgb.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#my_model = train_xgboost(feature_frame[[\"max_kmer_score\",\"dn_hg_score2\"]], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the created model to predict in a new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pssm = get_jaspar_pssm(\"MA0466.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_learning_data(feature_frame, pos_size, neg_size):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe with the features,\n",
    "    \"\"\"\n",
    "    \n",
    "    #import pandas as pd\n",
    "    #import numpy as np\n",
    "    #from sklearn.datasets import dump_svmlight_file\n",
    "    \n",
    "    y = np.concatenate((np.ones(pos_size), np.ones(neg_size)*-1), axis=0)\n",
    "\n",
    "    target = pd.Series.from_array(y)\n",
    "    target = target.apply(int)\n",
    "    target = target.to_frame(name=\"Target\")\n",
    "    \n",
    "    target_f1 = feature_frame.T.append(target.T).T\n",
    "\n",
    "\n",
    "    cutoff = target_f1.count()[0]\n",
    "\n",
    "    ids = pd.Series.from_array(np.arange(1, cutoff+1))\n",
    "    ids = ids.apply(int)\n",
    "\n",
    "    target_f1 = target_f1.T.append(ids.to_frame(name=\"Id\").T).T\n",
    "\n",
    "\n",
    "    X = target_f1[np.setdiff1d(target_f1.columns,['Id','Target'])]\n",
    "    y = target_f1.Target\n",
    "    \n",
    "    return target_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very disappointing performaces. Why so? Is it a failure in the scorin functions, the k-mers or the model, of the choice of negative sequences? What would happen if we added more features? Which features would these be? So, what then is the next step:\n",
    "- Expand my features to include PWM\n",
    "- Use the maximum k-mer score rather than the sum\n",
    "- test a different TFs\n",
    "- make the pos and negative set equal\n",
    "- Include counts in frequency difference directly, sclalled in the same way as E-scores\n",
    "- Test other machine learning aprroaches like SVM\n",
    "- Use a k-mer model from kmerSVM and use it to score the sequences\n",
    "\n",
    "Write up on what we will have learned so far. Spend the morning implementing the above and the afternoon in the cafe, writing up on where I am so far, especially the introduction and metholology section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trim_to_p = min(len(pos_bed_p), len(neg_bed_p))\n",
    "# #trim_to_p = 2000\n",
    "# pos_bed_p = pos_bed_p.head(trim_to_p)\n",
    "# neg_bed_p = neg_bed_p.head(trim_to_p)\n",
    "\n",
    "# combined_bed_p = pos_bed_p.append(neg_bed_p, ignore_index=True)\n",
    "\n",
    "# E_score_combined_p = get_kmer_score(combined_bed_p, sum_kmer_score, E_score_dict)\n",
    "# max_score_combined_p = get_kmer_score(combined_bed_p, max_score_kmer, E_score_dict)\n",
    "# dn_hg_score_combined2_p = get_kmer_score(combined_bed_p, max_score_kmer, dn_hg_dict2)\n",
    "# dn_hg_score_combined_p = get_kmer_score(combined_bed_p, max_score_kmer, dn_hg_dict)\n",
    "# dnase_scores_p = apply_get_max_dnase(combined_bed_p)\n",
    "\n",
    "# feature_frame_p = pd.DataFrame(E_score_combined_p, columns=[\"kmer_score\"])\n",
    "# feature_frame_p [\"max_kmer_score\"] = max_score_combined_p\n",
    "# feature_frame_p[\"dn_hg_score\"] = dn_hg_score_combined_p\n",
    "# feature_frame_p[\"dn_hg_score2\"] = dn_hg_score_combined2_p\n",
    "# feature_frame_p[\"dnase\"] = dnase_scores_p"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
